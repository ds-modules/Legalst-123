{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQwuKnYga1Ax"
   },
   "source": [
    "# [LEGALST-123] Lab 06: Regression and Causal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10v3Synra1Ay"
   },
   "source": [
    "This lab will review Ordinary Least Squares regression, the use of regression for causal inference, and interpreting regression models (including the idea of hypothesis testing). The idea here is to review how causal inference models are used in the social sciences (here, with data on bike rentals) and how to interpret those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocO4S2gca1Az"
   },
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z7Bj3tue7mx"
   },
   "source": [
    "**Here are some helpful resources to reference while doing this lab**:\n",
    "\n",
    "\n",
    "*   [Python Reference Table](https://www.data8.org/fa23/reference/)\n",
    "*   [Data 8 textbook - Regression](https://inferentialthinking.com/chapters/15/2/Regression_Line.html?highlight=regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWDAK61qa1A0"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc4VSJ72a1A0"
   },
   "source": [
    "The data we are exploring is collected from a bike sharing system in Washington D.C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "OKJWSYuPa1A1",
    "outputId": "916e1080-e9c8-4aab-cdc4-ac509b64dcaf"
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data, no further action is needed\n",
    "bike = pd.read_csv(\"https://github.com/ds-modules/data/raw/main/bikeshare.txt\")\n",
    "\n",
    "# Note that we're taking a random sample of the dataset since our dataset is large.\n",
    "bike = bike.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "bike.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGc8BxwaRIf_"
   },
   "source": [
    "The variables in this data frame are defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Description |\n",
    "| ---------| ------------|\n",
    "| instant  | record index|\n",
    "| dteday   | date |\n",
    "| season | 1: spring, 2: summer, 3: fall, 4: winter |\n",
    "| yr | 0: 2011, 1: 2012 |\n",
    "| mnth | month (1 to 12) |\n",
    "| hr | hour (0 to 23) |\n",
    "| holiday | whether the day is a holiday or not |\n",
    "| weekday | day of the week |\n",
    "| workingday | if the day is neither weekend nor holiday |\n",
    "| weathersit | 1: clear or partly cloudy, 2: mist and clouds, 3: light snow or rain, 4: heavy snow or rain |\n",
    "| temp | normalized temperature in Celsius (divided by 41) |\n",
    "| atemp | normalized \"feels-like\" temperature in Celsius (divided by 50) |\n",
    "| hum | normalized percent humidity (divided by 100) |\n",
    "| windspeed | normalized wind speed (divided by 67) |\n",
    "| casual | count of casual users |\n",
    "| registered | count of registered users |\n",
    "| cnt | count of total rental bikes, including casual and registered |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ZWpaqca1A3"
   },
   "source": [
    "## A Note on Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GphSPEKea1A3"
   },
   "source": [
    "Reflecting back on Lab 3, it’s crucial to remember the importance of cleaning our dataset to enhance the quality of our analysis. Below are some strategies that could be beneficial:\n",
    "\n",
    "\n",
    "1. Addressing Missing Data\n",
    "  *   Identify and handle missing data points to ensure they don’t negatively impact our analysis.\n",
    "  *   This could involve removing or imputing missing values depending on the situation. <br> <br>\n",
    "\n",
    "2. Recode Categorical Variables\n",
    "  *   Transform categorical variables into dichotomous variables, taking on values of 0 or 1, to enable analysis and interpretation. Be warned: sometimes categorical variables may take integer values, not strings. <br> <br>\n",
    "\n",
    "3. Standardize Scale\n",
    "  *   Ensure that all scales are recoded in a consistent direction, enhancing the interpretability of our results (we'll discuss the specifics of this later on!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fsPGUI-ewOT"
   },
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jNm2zc2OHYW"
   },
   "source": [
    "Recall from Data 8 that the least-squares regression line is the unique straight line that minimizes root mean squared error (RMSE) among all possible fit lines. Using this property, we can find the equation of the regression line by finding the pair of slope and intercept values that minimize the root mean squared error.\n",
    "\n",
    "**Simple linear regression** in this case refers to a specific type of least-squares regression in which we are only picking one independent variable and fitting a model to predict our dependent variable.\n",
    "\n",
    "For this example, we're going to explore the relationship between temperature (`temp`) and count (`cnt`). Let's do a simple linear regression with `\"temp\"` as a predictor for `\"cnt\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "W0fdvuS-fMVo",
    "outputId": "97b8f956-99f2-486e-8033-b90ba4f38d20"
   },
   "outputs": [],
   "source": [
    "# Preparing the data for modeling\n",
    "X = bike[['temp']]  # Predictor\n",
    "Y = bike['cnt']     # Response\n",
    "\n",
    "# Creating a scatter plot to visualize the data (without OLS Model First)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, Y, alpha=0.5)\n",
    "plt.title('Bike Rentals vs Temperature')\n",
    "plt.xlabel('Normalized Temperature (Celsius)')\n",
    "plt.ylabel('Count of Total Bike Rentals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based solely on the plot above, it looks like there may be a slight positive relationship between the temperature and count of bike rentals -- that is, generally, it seems like the number of bike rentals goes up as the temperature increases. To see if this is an accurate interpretation, we create the linear regression model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xM64HgjE6Qg"
   },
   "source": [
    "In order to fit the linear regression model, **we use the linear regression method from the scikit-learn package.** First, we **define our model as a linear regression** by calling the method, then we **fit our data with a predictor (X) and response (Y) variable.** We can then use the fitted model to predict the response variable and analyze how well it performs by comparing the predicted response value to the observed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "FN0n1ryefaYi",
    "outputId": "80611b84-a290-4347-a39a-ac6dd3b5b806"
   },
   "outputs": [],
   "source": [
    "# Fitting the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plotting the regression line\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, Y, alpha=0.5)  # actual points\n",
    "plt.plot(X, y_pred, color='red', linewidth=2)  # regression line\n",
    "plt.title('Bike Rentals vs Temperature')\n",
    "plt.xlabel('Normalized Temperature (Celsius)')\n",
    "plt.ylabel('Count of Total Bike Rentals')\n",
    "plt.show()\n",
    "\n",
    "# Output the model coefficients\n",
    "print(f\"Coefficient (slope): {model.coef_[0]}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Calculating and printing RMSE\n",
    "rmse = mean_squared_error(Y, y_pred, squared=False)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjUrl-H4hG5P"
   },
   "source": [
    "#### Analysis of Simple Linear Regression Results\n",
    "\n",
    "The coefficient, also known as the slope, indicates the change in the count of total bike rentals for each unit increase in normalized temperature. The positive value suggests a direct relationship between temperature and bike rentals. The intercept represents the expected count of bike rentals when the temperature is 0.\n",
    "\n",
    "However, it is important to consider the assumptions behind linear regression when interpreting these results. The main assumptions include linearity, independence, homoscedasticity, and normal distribution of residuals. If any of these assumptions do not hold, the predictions and inferences made from the model may be unreliable. Therefore, further analysis and diagnostics are necessary to validate these assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0Ul2jcfmbbg"
   },
   "source": [
    "**Steps of Linear Regression**\n",
    "\n",
    "Just like that, we've created a simple linear regression model! As we saw with the code above, it's a simple model to create and allows for interpretability of coefficients for us to understand clearly how our model makes its predictions.\n",
    "\n",
    "Here is a full breakdown of the steps to performing Ordinary Least Squares (OLS) regression:\n",
    "\n",
    "1. **Choose Your Variables**\n",
    "  * Decide on which variable you want to predict (the dependent variable) and which variables you will use to predict it (the independent variables). <br> <br>\n",
    "2. **Gather Data**\n",
    "  * Collect the data for all the variables you have decided to use. Ensure there are no missing values and that the data is clean. <br> <br>\n",
    "3. **Visualize Data**\n",
    "  * Plot the data to get a sense of the relationship between the independent and dependent variables. Look for trends, patterns, and potential outliers. <br> <br>\n",
    "4. **Fit the Model**\n",
    "  * Use the OLS method to find the coefficients that minimize the sum of the squares of the residuals (the differences between the observed values and the values predicted by the model). <br> <br>\n",
    "5. **Analyze Results**\n",
    "  * Examine the coefficients to see how changes in the independent variables are expected to affect the dependent variable. Check the R-squared value to see how well the model explains the variability of the dependent variable. <br> <br>\n",
    "6. **Diagnostic Checks (More on this at the bottom of the notebook)**\n",
    "  * Perform diagnostic tests to ensure the model assumptions are not violated. This may include checking for linearity, homoscedasticity, independence, and normality of residuals. <br> <br>\n",
    "7. **Make Predictions**\n",
    "  * Use the model to make predictions on new data, applying the coefficients to the independent variables to get predicted values for the dependent variable. <br> <br>\n",
    "8. **Evaluate the Model**\n",
    "  * Assess the model's performance using metrics like RMSE (Root Mean Squared Error) and validate it on a test dataset to ensure that it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qk5BJof1ckp"
   },
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, you saw an implementation of a simple linear regression model. In the following example, we'll take a look at another type of OLS model -- multiple linear regression. As the name implies, multiple linear regression allows us to predict a dependent variable as a linear combination of multiple independent variables.\n",
    "\n",
    "In this example, we'll also implement and break down many of the steps that we discussed in the outline of linear models above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 1 and 2: Choosing and Gathering Our Data\n",
    "\n",
    "For this example, we'll be predicting the `\"cnt\"` variable from the independent features `\"temp\"`, `\"hum\"`, `\"windspeed\"`, `\"season\"`, and `\"weathersit\"`. Let's gather all of these columns and set them up as variables, and we'll also double check that none of the values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a subset of variables for the multiple regression model\n",
    "features = ['temp', 'hum', 'windspeed', 'season', 'weathersit']  # example feature set\n",
    "X_multi = bike[features]\n",
    "y_multi = bike['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a combination of the .isna() and .sum() methods to make sure there are no missing values.\n",
    "X_multi.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_multi.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our independent and dependent variables set up in `X_multi` and `y_multi` respectively, and our data is null-free and ready to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Scaling and Splitting the Data\n",
    "\n",
    "We didn't explicitly mention this in the steps above, but two incredibly important steps with regards to treating our data before performing linear regression are scaling and splitting the data.\n",
    "\n",
    "Scaling the data refers to the process of **placing the numerical features on a standard scale.** If we skip this step before performing linear regression, it can potentially over- or underestimate the effect that our independent variables have on the dependent variable.\n",
    "\n",
    "Splitting the data refers to **dividing the data into two parts: a training set and a testing set.** We use the training set as the data that we actually fit the model with, and we use the testing set when we're looking into the accuracy or error rates of our predictions. It's crucial for us to take this step to avoid **overfitting our model, which is what happens when our model performs very well on our training data but poorly on unseen data.** We'll dive a bit deeper into this later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we use two functions from the scikit-learn package to easily do these tasks for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Visualize Data\n",
    "\n",
    "As in the simple linear regression model, let's look at the relationships between our independent and dependent variables before we do any model creation, in order to give us more insight into our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIS4CZ6Fm_wW"
   },
   "source": [
    "Take a look at the scatterplots we create below for some of the continuous independent variables against our dependent variable.\n",
    "\n",
    "Remember that:\n",
    "\n",
    "\n",
    "*   Continous variables can take any value within a range (e.g., temperature, height).\n",
    "*   Discrete variables can only take specific, separate values (e.g., count of people, number of cars).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dpBGV98pa1A7",
    "outputId": "3acbab82-58cb-40e7-dc71-4b5e7cdce5d0"
   },
   "outputs": [],
   "source": [
    "# Scatter plot for 'temp' vs 'cnt'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_train['temp'], y=y_train)\n",
    "plt.title('Temperature vs Total Bike Rentals')\n",
    "plt.xlabel('Normalized Temperature (Celsius)')\n",
    "plt.ylabel('Count of Total Bike Rentals')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for 'hum' vs 'cnt'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_train['hum'], y=y_train)\n",
    "plt.title('Humidity vs Total Bike Rentals')\n",
    "plt.xlabel('Normalized Humidity')\n",
    "plt.ylabel('Count of Total Bike Rentals')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for 'windspeed' vs 'cnt'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_train['windspeed'], y=y_train)\n",
    "plt.title('Windspeed vs Total Bike Rentals')\n",
    "plt.xlabel('Normalized Windspeed')\n",
    "plt.ylabel('Count of Total Bike Rentals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features above seem like they might be suitable for linear regression, while others seem like they aren't quite as applicable. Understanding these trends before creating a model can be helpful in providing more context for the results of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 4, 7, and 8: Fitting the Model, Making Predictions, and Evaluating the Model\n",
    "\n",
    "We'll combine some of the steps specified above because they go together quite well and are easy to implement thanks to scikit-learn.\n",
    "\n",
    "As before, we'll define the model, fit it to our training data, make predictions for our test data, and then see how the model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EJsG_kNa1A6",
    "outputId": "6fd08d0e-f02f-40b9-8458-f163a00e9470"
   },
   "outputs": [],
   "source": [
    "# Fitting the multiple regression model\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_multi = model_multi.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the model\n",
    "rmse_multi = mean_squared_error(y_test, y_pred_multi, squared=False)\n",
    "print(f\"Root Mean Squared Error (Multiple Regression): {rmse_multi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the RMSE for this model is 145.77, while the simple linear regression model had an RMSE of 160.59! This model seems to do a better job of making predictions for our data. However, this brings us to an important point to be cautious of when performing OLS regression: the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances\n",
    "Usually, when we are testing a model developed from social science theory we want to evaluate causal importance of the features (independent variables). Since we have standardized the features, we cannot easily evaluate them in terms of their all-else-equal effect on the outcome (although we could if we wanted use the unstandardized features and merely rescale them to be comparable). But we can investigate their relative importance using sklearn. **Think especially about the `weathersit` and `season` variables in the dataset. What kind of variables are they? Should we be using them here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the standardized regression coefficients for the features in our multiple regression model\n",
    "# and the columns that made up X\n",
    "# and put them into a df\n",
    "standardized_coeffs = pd.DataFrame...\n",
    "standardized_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question:**</span><p>\n",
    "_**Does the list of standardized coefficients for the features in the model make sense? Should we have used `weathersit` and `season` in our model like this?** Explain here_ <p>\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff is a significant consideration when creating models for regression (or any type of value prediction tasks). As discussed above, overfitting is the situation in which a model performs very well on the trained data but poorly on unseen data. Model overfitting can happen in two major ways:\n",
    "\n",
    "1. **Not splitting the data into training and testing sets.**\n",
    "    * The model that we create is the one that performed the best on the training set. If we use all of our data to train the model, it will become very good at formulating predictions for the data that it saw and was trained on, but it will not be able to make accurate predictions for any seen data. Essentially, the model will not be generalizable. <br> <br>\n",
    "    \n",
    "2. **Utilizing too many features as independent variables in the model.**\n",
    "    * The tradeoff between having high accuracy and being generalizable is called the bias-variance tradeoff. Let's look at an example of this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a multiple linear regression model using 11 of the independent variables in our dataset. Along with this, in order to show the issue with using this many features, we'll change the sizes of our training and testing split between our data: in this case, we'll train on only a bit of the data and use a lot of it for testing.\n",
    "\n",
    "Below, we'll perform the exact same process that we did before, but we'll keep all of the code in one big cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a subset of variables for the multiple regression model\n",
    "features_of = ['temp', 'hum', 'windspeed', 'season', 'weathersit', 'holiday', 'workingday', 'mnth', 'hr', 'weekday', 'atemp']\n",
    "X_multi_of = bike[features_of]\n",
    "y_multi_of = bike['cnt']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train_of, X_test_of, y_train_of, y_test_of = train_test_split(X_multi_of, y_multi_of, test_size=0.75, random_state=42)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_of = scaler.fit_transform(X_train_of)\n",
    "X_test_scaled_of = scaler.transform(X_test_of)\n",
    "\n",
    "# Fitting the multiple regression model\n",
    "model_multi_of = LinearRegression()\n",
    "model_multi_of.fit(X_train_scaled_of, y_train_of)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_multi_of = model_multi_of.predict(X_test_scaled_of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our new model, **let's compare the RMSEs for this model and the previous multiple LR model for the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "rmse_multi = mean_squared_error(y_train, model_multi.predict(X_train_scaled), squared=False)\n",
    "rmse_multi_of = mean_squared_error(y_train_of, model_multi_of.predict(X_train_scaled_of), squared=False)\n",
    "print(f\"Root Mean Squared Error (Multiple Regression): {rmse_multi}\")\n",
    "print(f\"Root Mean Squared Error (Multiple Regression Overfit): {rmse_multi_of}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're comparing the RMSE of the two models on our training set, we see that the model with more features does a bit better than the one with less features! But now, let's take a look at the RMSEs of the two models on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "rmse_multi = mean_squared_error(y_test, model_multi.predict(X_test_scaled), squared=False)\n",
    "rmse_multi_of = mean_squared_error(y_test_of, model_multi_of.predict(X_test_scaled_of), squared=False)\n",
    "print(f\"Root Mean Squared Error (Multiple Regression): {rmse_multi}\")\n",
    "print(f\"Root Mean Squared Error (Multiple Regression Overfit): {rmse_multi_of}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the issue with using this many features for prediction: because we saved a lot of our data for testing rather than training, we could see how our model became overfit on the training data due to all of the features that we used. Be sure to keep this tradeoff in mind as you work on regression problems in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQnvDS5ba1A8"
   },
   "source": [
    "## ✅ Question 1: Try Your Own Model on the Dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbkQiI7Ma1A8"
   },
   "source": [
    "Now that we've run through a simple linear regression example and a multiple linear regression example, try one of your own! Come up with a explanatory model for the bike rentals dataset or relationship you are interested in exploring using linear regression. **In the following text cell, outline the model including the theory of human behavior (however basic) that underlies your model.** Then, implement your model in the code cell below. Finally, in another text cell, describe what you found with your model.\n",
    "\n",
    "**Note:** Feel free to add additional code cells if you'd prefer to split up the steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWLUCV7ma1A9"
   },
   "source": [
    "*Discuss your model in words here!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DwIRM1Ma1A9"
   },
   "outputs": [],
   "source": [
    "# Implement your model here, commenting on the steps you take to clean the data and make the variables\n",
    "# ready to use in an OLS regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ-5c8d9a1A9"
   },
   "source": [
    "*Here, describe what you found with your model (remember that a negative finding is still a finding!).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqSJ_J5Fa1A6"
   },
   "source": [
    "## Some Parting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Continuous vs. Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples in this lab, we predicted a continuous variable -- the count of bike rentals. However, if our problem instead wanted to predict a discrete variable, such as whether a particular individual rented a bike (1) or not (0), that would require a different approach than regression: **classification**, or the prediction task of classifying units into some group or another. We'll explore this a bit more in future labs, so stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OLS Regression to Make an Inference from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahFMsBT2hoJZ"
   },
   "source": [
    "In the social sciences, regression is often used to support a causal argument based on some kind of theory of human action.\n",
    "In effect, what it represents mathematically is, \"What is the effect of a unit change in  𝑥  (the cause of interest) on  𝑦  (the outcome of interest), holding everything else equal?\"\n",
    "Another way of thinking about it is as the partial correlation between a single variable  𝑥  and an outcome  𝑦. Note that when we use `standard scaler` (which expresses variables in terms of their mean and variance) we cannot make direct inferences about the effect of a unit change in _x_ on a unit change in _y_. When you read social science articles, you will note that they will transform variables such that they are on a scale of 0 to 1 in order for the reader to more easily interpret what a change in _x_ will do to _y_.\n",
    "\n",
    "Correlation and regression are introduced quite well in the Data 8 textbook linked at the top of this notebook, so in this notebook we are trying to show how social scientists use regression to make causal arguments. We asked you to include a very simple theory of human behavior to motivate your model-building above. Social scientists draw on theory to explain the causal mechanism behind the association of the input variables and the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSJ5Itweh7zy"
   },
   "source": [
    "As we discussed above, there are a number of assumptions that regression techniques require that we will keep returning to in future notebooks. Here's a list of the major ones:\n",
    "* Independent variables (IVs) are quantitative or dichotomous.\n",
    "* Dependent variable is quantitative, continuous, and unbounded.\n",
    "* All IVs have variance not equal to 0 (ie. there is some variation in their value).\n",
    "* No perfect multicollinearity between any two IVs.\n",
    "* For each set of values for the independent variables, the mean value of the error term is zero.\n",
    "* Each IV is uncorrelated with the error term.\n",
    "* The variance of the error term for each set of values for the IVs is constant (homoscedasticity assumption).\n",
    "* Error terms for any two observations of the values of IVs are uncorrelated.\n",
    "* Error terms for each set of values for the IVs are normally distributed.\n",
    "\n",
    "Some of these assumptions may not have been satisfied by the examples that we did above and are out of scope for this class -- that's okay. However, in future work, it's important for us to keep these in mind and do our best to satisfy them or identify where we fell short with them, in order to provide the proper justification for our model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
