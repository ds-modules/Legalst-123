{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d83200c",
   "metadata": {},
   "source": [
    "## Lab 11: Data Cleaning and Exploratory Data Analysis for Classification Problems\n",
    "\n",
    "This lab will go through a little more exploration of the Nashville traffic stop data. _This time we will use a sample of cleaned data._ <span style=\"color:orange\">**We will also apply the mapping tools from the preceding labs to some of the data, e.g., for stops with searches by individual officers.**</span>\n",
    "\n",
    "Even though it is an unusual outcome, in this lab we are going to consider searches as our outcome of interest. We will focus on learning the data, cleaning the data, and exploring it. Subsequent labs will move on to predictive models.\n",
    "\n",
    "Documentation for police traffic stop data for Nashville https://github.com/stanford-policylab/opp/blob/master/data_readme.md#nashville-tn is drawn from the Stanford traffic stop data library. \n",
    "\n",
    "There are some materials online that can help us put Nashville policing practices for traffic stops in context. Remember that Nashville is a \"blue\" city in a \"red\" state, and that whenever we see dramatic changes over time we want to see whether there was an important policy change.<p>\n",
    "* https://policylab.stanford.edu/media/nashville-traffic-stops.pdf <p>\n",
    "* https://filetransfer.nashville.gov/portals/0/sitecontent/CommunityOversight/docs/2019-1023/TrafficStopResearchProposal.pdf<p>\n",
    "* https://www.policingproject.org/nashville<p>\n",
    "* https://www.tennessean.com/story/news/crime/2019/04/18/nashville-traffic-stops-police-study-statistics-driving-while-black/3273143002/<p>\n",
    "* https://www.tennessean.com/story/news/crime/2018/11/20/nashville-traffic-stop-report-policing-project-bias-jocques-clemmons/2066165002/<p>\n",
    "* https://www.davisvanguard.org/2021/04/traffic-stop-assessment-by-nyu-law-in-nashville-may-be-applicable-to-other-u-s-cities/<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed04961",
   "metadata": {},
   "source": [
    "### from the readme file on the Nashville dataset (2010-01-01 to 2019-03-24)\n",
    "#### Data notes:\n",
    "*    Data is deduplicated on raw columns stop_date_time, stop_location_street, officer_employee_number, race, sex, and age_of_suspect, reducing the number of records by ~0.3%\n",
    "*    There are 30 (of ~2.6M records) cases where search_conducted is ambiguous after the merge and are left as NA, since it's unclear whether they are true or false, since being NA after the above merge indicates that there were two distinct values for raw column searchoccur\n",
    "*    reason_for_stop and violation are both translations of the original stop_type column; this column is sometimes the pretextual reason for the stop and does not always represent what the individual was ultimately cited for\n",
    "*    contraband_drugs is raw column drugs_seized, contraband_weapons is weapons_seized, and contraband_found is evidenceseized\n",
    "*    citation_issued is derived from traffic_citation_issued and misd_state_citation_issued, which are passed through as raw_*; misd_state_citation_issued is sometimes NA, so for the purposes of defining citation_issued, we consider NA to be false\n",
    "*    warning_issued is derived from verbal_warning_issued and written_warning_issued, which are passed through as raw_*; written_warning_issued is sometimes NA, so for the purposes of defining warning_issued, we consider NA to be false\n",
    "*    search_basis is based on the raw columns search_plain_view, search_consent, search_incident_to_arrest, search_warrant, and search_inventory, which are all passed on with the raw_* prefix\n",
    "*    subject_race is derived from raw columns suspect_ethnicity and suspect_race, which are passed through with the raw_* prefix\n",
    "*    search_person is derived from search_driver and search_passenger, which are passed through with the raw_* prefix\n",
    "*    When contraband_found is NA, we fill it with false when a search occurred, under the assumption that the officer simply didn't record the absence of contraband\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13072ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /Users/jdmarshall/anaconda3/lib/python3.11/site-packages (5.9.0)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/jdmarshall/anaconda3/lib/python3.11/site-packages (from plotly) (8.2.2)\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML # makes the output in Jupyter notebook pretty\n",
    "!pip install plotly\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "# !pip install folium\n",
    "import json\n",
    "import os\n",
    "import folium.plugins # The Folium Javascript Map Library\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime\n",
    "from folium.plugins import FastMarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03861bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>location</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>precinct</th>\n",
       "      <th>reporting_area</th>\n",
       "      <th>zone</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>officer_id_hash</th>\n",
       "      <th>...</th>\n",
       "      <th>violation_moving traffic violation</th>\n",
       "      <th>violation_parking violation</th>\n",
       "      <th>violation_registration</th>\n",
       "      <th>violation_safety violation</th>\n",
       "      <th>violation_seatbelt violation</th>\n",
       "      <th>violation_vehicle equipment violation</th>\n",
       "      <th>subject_race</th>\n",
       "      <th>subject_sex</th>\n",
       "      <th>violation</th>\n",
       "      <th>stardate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_row_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469050</th>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>10:52:00</td>\n",
       "      <td>924 CURREY RD, NASHVILLE, TN, 37217</td>\n",
       "      <td>36.108380</td>\n",
       "      <td>-86.708519</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8831.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37810c10d6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>2016-05-01 10:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210355</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>17:53:00</td>\n",
       "      <td>10TH AVE S &amp; MONTROSE AVE, NASHVILLE, TN, 37204</td>\n",
       "      <td>36.123393</td>\n",
       "      <td>-86.786429</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>823.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>cf72c2298a</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>2013-02-01 17:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412047</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>11:35:00</td>\n",
       "      <td>1900 HOBSON PIKE, ANTIOCH, TN, 37013</td>\n",
       "      <td>36.046237</td>\n",
       "      <td>-86.598773</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8927.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>d2c8e20a5a</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>female</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>2011-04-01 11:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326635</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>10:04:00</td>\n",
       "      <td>ERIN LN &amp; HWY 70 S, NASHVILLE, TN, 37221</td>\n",
       "      <td>36.078407</td>\n",
       "      <td>-86.908912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4901.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0a70309850</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>2015-12-01 10:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125061</th>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>12:32:00</td>\n",
       "      <td>BRILEY PKWY N &amp; MURFREESBORO PIKE, NASHVILLE, ...</td>\n",
       "      <td>36.122074</td>\n",
       "      <td>-86.702419</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8998.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>83fbfcfd39</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>female</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>2012-12-01 12:32:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date      time  \\\n",
       "raw_row_number                         \n",
       "2469050         2016-05-01  10:52:00   \n",
       "1210355         2013-02-01  17:53:00   \n",
       "412047          2011-04-01  11:35:00   \n",
       "2326635         2015-12-01  10:04:00   \n",
       "1125061         2012-12-01  12:32:00   \n",
       "\n",
       "                                                         location        lat  \\\n",
       "raw_row_number                                                                 \n",
       "2469050                       924 CURREY RD, NASHVILLE, TN, 37217  36.108380   \n",
       "1210355           10TH AVE S & MONTROSE AVE, NASHVILLE, TN, 37204  36.123393   \n",
       "412047                       1900 HOBSON PIKE, ANTIOCH, TN, 37013  36.046237   \n",
       "2326635                  ERIN LN & HWY 70 S, NASHVILLE, TN, 37221  36.078407   \n",
       "1125061         BRILEY PKWY N & MURFREESBORO PIKE, NASHVILLE, ...  36.122074   \n",
       "\n",
       "                      lng  precinct  reporting_area   zone  subject_age  \\\n",
       "raw_row_number                                                            \n",
       "2469050        -86.708519       3.0          8831.0  315.0         52.0   \n",
       "1210355        -86.786429       8.0          6949.0  823.0         29.0   \n",
       "412047         -86.598773       3.0          8927.0  335.0         18.0   \n",
       "2326635        -86.908912       1.0          4901.0  121.0         65.0   \n",
       "1125061        -86.702419       5.0          8998.0  531.0         20.0   \n",
       "\n",
       "               officer_id_hash  ... violation_moving traffic violation  \\\n",
       "raw_row_number                  ...                                      \n",
       "2469050             37810c10d6  ...                                  1   \n",
       "1210355             cf72c2298a  ...                                  1   \n",
       "412047              d2c8e20a5a  ...                                  1   \n",
       "2326635             0a70309850  ...                                  1   \n",
       "1125061             83fbfcfd39  ...                                  1   \n",
       "\n",
       "                violation_parking violation  violation_registration  \\\n",
       "raw_row_number                                                        \n",
       "2469050                                   0                       0   \n",
       "1210355                                   0                       0   \n",
       "412047                                    0                       0   \n",
       "2326635                                   0                       0   \n",
       "1125061                                   0                       0   \n",
       "\n",
       "                violation_safety violation violation_seatbelt violation  \\\n",
       "raw_row_number                                                            \n",
       "2469050                                  0                            0   \n",
       "1210355                                  0                            0   \n",
       "412047                                   0                            0   \n",
       "2326635                                  0                            0   \n",
       "1125061                                  0                            0   \n",
       "\n",
       "                violation_vehicle equipment violation  subject_race  \\\n",
       "raw_row_number                                                        \n",
       "2469050                                             0         white   \n",
       "1210355                                             0         white   \n",
       "412047                                              0       unknown   \n",
       "2326635                                             0         white   \n",
       "1125061                                             0       unknown   \n",
       "\n",
       "                subject_sex                 violation             stardate  \n",
       "raw_row_number                                                              \n",
       "2469050              female  moving traffic violation  2016-05-01 10:52:00  \n",
       "1210355                male  moving traffic violation  2013-02-01 17:53:00  \n",
       "412047               female  moving traffic violation  2011-04-01 11:35:00  \n",
       "2326635              female  moving traffic violation  2015-12-01 10:04:00  \n",
       "1125061              female  moving traffic violation  2012-12-01 12:32:00  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "path = \"https://github.com/ds-modules/data/raw/main/nashville_cleaned_sample.csv\"\n",
    "cleaned_stops = pd.read_csv(path, index_col=0)\n",
    "cleaned_stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183602d",
   "metadata": {},
   "source": [
    "### 1. Dataset basics: some facts, some exploration\n",
    "First we will just get a handle on the sample of cleaned Nashville traffic stop data by finding its shape, what columns are in it, what the proportions of missing data are for each column, and what the searches look like geographically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b12cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shape of the dataframe, what the columns are, what the data type in each column is, etc.\n",
    "# YOUR CODE HERE (feel free to use multiple code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b31069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show proportions of missing data\n",
    "pd.options.display.float_format = '{:.8f}'.format # use 8 decimal places and not scientific notation for float\n",
    "# cleaned_stops ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cc226",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning, Dealing with Missing Values, Creating Dummy Variables\n",
    "We covered data cleaning and missing values in Labs 3-6, so this is just a brief review. The sample of cleaned data that we are using in this lab already has dichotomous (dummy) variables that indicate subject race, subject sex, and type of violation that resulted in the stop, but if you look closely the dataset retains the original variables--I think it is always a good idea not to throw away information if possible. In this section we will get rid of the existing dummy variables and create new ones and decide on what to do with missing values in the process of readying data for predictive models (which we will create in Labs 12-14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a0d0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, 2469050 to 2325584\n",
      "Data columns (total 31 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   date                        2000 non-null   object \n",
      " 1   time                        2000 non-null   object \n",
      " 2   location                    2000 non-null   object \n",
      " 3   lat                         2000 non-null   float64\n",
      " 4   lng                         2000 non-null   float64\n",
      " 5   precinct                    1806 non-null   float64\n",
      " 6   reporting_area              1849 non-null   float64\n",
      " 7   zone                        1806 non-null   float64\n",
      " 8   subject_age                 2000 non-null   float64\n",
      " 9   officer_id_hash             2000 non-null   object \n",
      " 10  type                        2000 non-null   object \n",
      " 11  arrest_made                 2000 non-null   bool   \n",
      " 12  citation_issued             2000 non-null   bool   \n",
      " 13  warning_issued              2000 non-null   bool   \n",
      " 14  outcome                     2000 non-null   object \n",
      " 15  contraband_found            92 non-null     float64\n",
      " 16  contraband_drugs            92 non-null     float64\n",
      " 17  contraband_weapons          92 non-null     float64\n",
      " 18  frisk_performed             2000 non-null   bool   \n",
      " 19  search_conducted            2000 non-null   bool   \n",
      " 20  search_person               2000 non-null   bool   \n",
      " 21  search_vehicle              2000 non-null   bool   \n",
      " 22  search_basis                92 non-null     object \n",
      " 23  vehicle_registration_state  1978 non-null   object \n",
      " 24  notes                       324 non-null    object \n",
      " 25  year                        2000 non-null   int64  \n",
      " 26  month                       2000 non-null   int64  \n",
      " 27  subject_race                2000 non-null   object \n",
      " 28  subject_sex                 2000 non-null   object \n",
      " 29  violation                   2000 non-null   object \n",
      " 30  stardate                    2000 non-null   object \n",
      "dtypes: bool(7), float64(9), int64(2), object(13)\n",
      "memory usage: 404.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# get rid of the existing dummy variables constructed from\n",
    "# 'violation', 'subject_race', and 'subject_sex' but keep working with the cleaned_stops dataframe\n",
    "# confirm that the resulting dataframe is what we expect\n",
    "\n",
    "dummy_cols = ...\n",
    "cleaned_stops.drop...\n",
    "cleaned_stops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d9b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain's log supplemental. The first entry in 'stardate' is  2016-05-01 10:52:00 and is of type  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# what's that last feature?\n",
    "print(\"Captain's log supplemental. The first entry in 'stardate' is \", cleaned_stops.stardate[0], \"and is of type \", type(cleaned_stops.stardate[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae3b29",
   "metadata": {},
   "source": [
    "Now we are back to something like the original configuration of the data. The features \"year\", \"month\", and \"stardate\" were created from the datetime features \"date\" and \"time\" and \"year\" and \"month\" are integer representations of the year and month of the stop. Note that you can use a dot notation for columns of a dataframe, although it is not as clear as the brackets for a column index.\n",
    "\n",
    "**Question:** Do we need to do anything with the with the large number of missing values for the \"contraband_\"  and \"search_basis\" features? How about \"notes\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ecc7c7",
   "metadata": {},
   "source": [
    "_Your answer here_<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134a88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the operations necessary to deal with missing data, if they are needed, and add a note of explanation\n",
    "# in a markdown cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d635d4b",
   "metadata": {},
   "source": [
    "_your explanation here_\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4260e",
   "metadata": {},
   "source": [
    "### Creating dummy variables from categorical variables\n",
    "We just took the dummy variables out of the dataset, but it is important to be able to create them when you have categorical variables that you wish to use in a model. We are not going to do prediction exercises in this lab, but we will in the next several labs, in the unit on computational text analysis, in the homework, and in the project. Luckily, Pandas makes this part easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dichotomous variables to represent categorical variables in the dataset, that is,\n",
    "# 'subject_race', 'subject_sex', and 'violation', and then check the resulting dataframe\n",
    "\n",
    "categorical_cols = ...\n",
    "cleaned_stops_dummies = pd.get_dummies...\n",
    "cleaned_stops_dummies..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ace294",
   "metadata": {},
   "source": [
    "**Take a close look at the new dataframe** we just made that has dummy variables for each category in the categorical variables. It's missing the original categorical variables. What if we want to keep them in the dataframe for some purpose (like ease of making boxplots)?\n",
    "\n",
    "Let's add the categorical variables themselves back to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e9dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the categorical variables back to the dataframe and check to make sure we got what we expect\n",
    "# note: if you add back the columns using the function pd.concat, Pandas will turn the integers and booleans\n",
    "# into floats, which will complicate things later, so use the .join method instead\n",
    "\n",
    "\n",
    "save_cols = ...\n",
    "cleaned_stops = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcf8f9",
   "metadata": {},
   "source": [
    "### 3. Some EDA Using Maps\n",
    "Mapping is an excellent way to explore data that has location features, and in the Nashville traffic stop data we have latitude and longitude, as well as date and time. We can get an idea of the spatial distribution of traffic stops, and even some idea of how individual police officers make traffic stops. Refer to the mapping labs, and feel free to think of other things you would like to map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a basemap of Nashville using your choice of tiles\n",
    "# note that \"Stamen Toner\" works better with the markers we use below\n",
    "# but that \"Stamen Toner\" will no longer render unless it is on Datahub\n",
    "# so try for something that will still appear in the saved file, like 'Open Street Map'\n",
    "# the Folium documentation is at https://python-visualization.github.io/folium/latest/user_guide/raster_layers/tiles.html#Other-tilesets\n",
    "\n",
    "\n",
    "nashville_coords = (36.174465, -86.767960)\n",
    "\n",
    "nashville_map =folium.Map(location=\n",
    "                              nashville_coords, \n",
    "                              zoom_start=12, \n",
    "                              tiles=...\n",
    "                              attr=...\n",
    "                         )\n",
    "nashville_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf8e6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe filtering for stops that resulted in searches and count how many searches happened in each year\n",
    "\n",
    "searches = ...\n",
    "searches['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08ecf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a heatmap of the searches in the dataset\n",
    "# to do this we first need create an array of latitudes, then of longitudes\n",
    "# then pass those to folium in the format it expects\n",
    "lats = searches['lat'].values\n",
    "longs = ...\n",
    "search_locs = np.vstack...\n",
    "search_locs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make a heatmap of locations where searches happened\n",
    "\n",
    "heatmap = ...\n",
    "heatmap.add_to ...\n",
    "nashville_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf304be4",
   "metadata": {},
   "source": [
    "**How informative is the heatmap of Nashville traffic stops that resulted in searches? Can you make any generalizations about searches from what you see?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca4730",
   "metadata": {},
   "source": [
    "_Your answer here:_<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10934b41",
   "metadata": {},
   "source": [
    "#### Changing Unit of Analysis to Individual Nashville PD Officers\n",
    "Instead of using Nashville as our unit of analysis, we can examine searches by individual officers to see if we can better understand any patterns in the data. Our unit of observation remains individual stops. Here we will first make a table of the top 10 officers in terms of searches over the time period in the dataset and aggregate each column in the data in a way that makes sense. After that, we will try mapping the stops resulting in searches by particular officers. _This would look more interesting if we used all the data in the dataset rather than just our sample, because searches are an unusual outcome._\n",
    "\n",
    "First, make a table of searches by officer, for the top 10 officers in descending order by number of searches, aggregating each feature in a meaningful way. For example, you would want to aggregate `'subject_age'` as a mean, rather than a sum, but for other features you may want to report both a sum and a mean (which for dichotomous variables is a proportion, remember). You should only use features that are summable (i.e., dichotomous features are fine, categorical ones are not).\n",
    "* Hint 1: the sample data is not so voluminous so you can make a new dataframe with the columns you select\n",
    "* Hint 2: when you aggregate by officer, you can pass [.agg a dictionary](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html#pandas.core.groupby.DataFrameGroupBy.agg) telling it what to do with each column\n",
    "* Hint 3: you can sort the table by `'search_conducted'` and [the aggregation function index label 'sum'](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values) thusly `by=('search_conducted','sum')` since Pandas sees that now as a tuple\n",
    "\n",
    "The last bit of code in the cell renders the table and displays it nicely in the notebook. Note that the means and sums are just for the searches by officer caught in our sample of 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9429746a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>subject_age</th>\n",
       "      <th colspan=\"2\" halign=\"left\">arrest_made</th>\n",
       "      <th colspan=\"2\" halign=\"left\">citation_issued</th>\n",
       "      <th colspan=\"2\" halign=\"left\">warning_issued</th>\n",
       "      <th colspan=\"2\" halign=\"left\">search_conducted</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_race_asian/pacific islander</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_race_black</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_race_hispanic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_race_other</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_race_unknown</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_race_white</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_sex_female</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subject_sex_male</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_child restraint</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_investigative stop</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_moving traffic violation</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_parking violation</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_registration</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_safety violation</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_seatbelt violation</th>\n",
       "      <th colspan=\"2\" halign=\"left\">violation_vehicle equipment violation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>officer_id_hash</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82f2e31c2b</th>\n",
       "      <td>28.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11111111</td>\n",
       "      <td>2</td>\n",
       "      <td>0.22222222</td>\n",
       "      <td>8</td>\n",
       "      <td>0.88888889</td>\n",
       "      <td>8</td>\n",
       "      <td>0.88888889</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11111111</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.77777778</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.22222222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0418967821</th>\n",
       "      <td>31.33333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b7a7efb70</th>\n",
       "      <td>29.75000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccda12115e</th>\n",
       "      <td>38.28571429</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14285714</td>\n",
       "      <td>2</td>\n",
       "      <td>0.28571429</td>\n",
       "      <td>6</td>\n",
       "      <td>0.85714286</td>\n",
       "      <td>2</td>\n",
       "      <td>0.28571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14285714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14285714</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.71428571</td>\n",
       "      <td>3</td>\n",
       "      <td>0.42857143</td>\n",
       "      <td>4</td>\n",
       "      <td>0.57142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14285714</td>\n",
       "      <td>4</td>\n",
       "      <td>0.57142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.28571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1d2836b079</th>\n",
       "      <td>38.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6761423fb7</th>\n",
       "      <td>36.33333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11111111</td>\n",
       "      <td>13</td>\n",
       "      <td>0.72222222</td>\n",
       "      <td>4</td>\n",
       "      <td>0.22222222</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11111111</td>\n",
       "      <td>10</td>\n",
       "      <td>0.55555556</td>\n",
       "      <td>6</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>12</td>\n",
       "      <td>0.66666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.22222222</td>\n",
       "      <td>14</td>\n",
       "      <td>0.77777778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393639dfae</th>\n",
       "      <td>33.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00443cb79c</th>\n",
       "      <td>29.25000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb792661d8</th>\n",
       "      <td>41.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1b55d714e1</th>\n",
       "      <td>28.37500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12500000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.87500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.87500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12500000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.87500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's aggregate searches by individual officers to find indiv officers with most searches over period\n",
    "\n",
    "# SOLUTION\n",
    "agg_stops = pd.DataFrame(data=cleaned_stops, columns=['subject_age', 'arrest_made',\n",
    "       'citation_issued', 'warning_issued','search_conducted','year', 'month',\n",
    "       'subject_race_asian/pacific islander', 'subject_race_black', 'officer_id_hash',\n",
    "       'subject_race_hispanic', 'subject_race_other', 'subject_race_unknown',\n",
    "       'subject_race_white', 'subject_sex_female', 'subject_sex_male',\n",
    "       'violation_child restraint', 'violation_investigative stop',\n",
    "       'violation_moving traffic violation', 'violation_parking violation',\n",
    "       'violation_registration', 'violation_safety violation',\n",
    "       'violation_seatbelt violation', 'violation_vehicle equipment violation'])\n",
    "officer_stops = agg_stops.groupby(by=['officer_id_hash']).agg({'subject_age':'mean','arrest_made':['sum','mean'],\n",
    "                                                                'citation_issued':['sum','mean'], \n",
    "                                                                'warning_issued':['sum','mean'],\n",
    "                                                                'search_conducted':['sum','mean'],\n",
    "                                                                'subject_race_asian/pacific islander':['sum','mean'], \n",
    "                                                                'subject_race_black':['sum','mean'], \n",
    "                                                                'subject_race_hispanic':['sum','mean'], \n",
    "                                                                'subject_race_other':['sum','mean'], \n",
    "                                                                'subject_race_unknown':['sum','mean'],\n",
    "                                                                'subject_race_white':['sum','mean'], \n",
    "                                                                'subject_sex_female':['sum','mean'], \n",
    "                                                                'subject_sex_male':['sum','mean'],\n",
    "                                                                'violation_child restraint':['sum','mean'], \n",
    "                                                                'violation_investigative stop':['sum','mean'],\n",
    "                                                                'violation_moving traffic violation':['sum','mean'], \n",
    "                                                                'violation_parking violation':['sum','mean'],\n",
    "                                                                'violation_registration':['sum','mean'], \n",
    "                                                                'violation_safety violation':['sum','mean'],\n",
    "                                                                'violation_seatbelt violation':['sum','mean'], \n",
    "                                                                'violation_vehicle equipment violation':['sum','mean']\n",
    "                                                                })\n",
    "table = officer_stops.sort_values(by=('search_conducted','sum'),ascending=False)[:10]\n",
    "display(HTML(table.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e221720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 47)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can extract into a list the top 10 officers in terms of searches and \n",
    "# put them into their own dataframe to use in mapping\n",
    "\n",
    "top_ten = ...\n",
    "\n",
    "# hint: use the slicing method .loc to get just the rows where the Nashville PD officer .isin the top_ten\n",
    "\n",
    "top_ten_map_df = ...\n",
    "top_ten_map_df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk through the df to assign a color to each officer (i.e. row)\n",
    "# this is probably way inefficient; if you know a more efficient way, by all means use it\n",
    "\n",
    "color_list=[]\n",
    "for row in top_ten_map_df.itertuples():\n",
    "    for badge in top_ten:\n",
    "        if badge==row.officer_id_hash: \n",
    "            color_list.append(top_ten.index(badge))\n",
    "                  \n",
    "top_ten_map_df['color']=color_list  \n",
    "\n",
    "# take the index and (lat,lon) pairs from df and map them\n",
    "colors=['red', 'blue', 'green', 'purple', 'orange', 'white', 'pink',\n",
    "         'lightgreen', 'gray', 'black', 'lightgray']\n",
    "\n",
    "for row in top_ten_map_df.itertuples(index=False):\n",
    "    folium.Circle(\n",
    "            location=[row.lat, row.lng], \n",
    "            popup=(row.year,row.violation,row.officer_id_hash,row.subject_race),\n",
    "            radius=10,           \n",
    "            color=colors[row.color],  # the color column is an integer that indexes colors list in this cell  \n",
    "            fill=True,\n",
    "            fill_color=colors[row.color]            \n",
    "        ).add_to(nashville_map)\n",
    "nashville_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09584d",
   "metadata": {},
   "source": [
    "**Question:** How interesting is the map of searches by the top 10 NPD searching officers? About how many points would there be on the map if we had used the entire dataset of about 2.8M traffic stops?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef50e95",
   "metadata": {},
   "source": [
    "_Your answer here_ <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae501e5",
   "metadata": {},
   "source": [
    "### 4. Feature Importance and Feature Selection\n",
    "Before we try to train any models to classify the Nashville traffic stops along some dimension (e.g., whether or not the stop resulted in a search), we need to think a bit about how the features in the dataset relate to the outcome of interest. There are several steps in this process.\n",
    "* impute missing values for rows where we have the information to replace a NaN value (e.g. `'contraband_found'`)\n",
    "* remove features that are non-numeric so that we can create matrix of bivariate correlations\n",
    "* examine the bivariate correlations among all the features to see if there are some good candidate predictors for our outcome and check that they are not collinear\n",
    "* make sure that none of the likely predictors are logically posterior to the outcome we are trying to predict. \n",
    "The last point is often talked about in Machine Learning as \"data leakage.\" The basic idea is that you cannot use a predictor that requires you to know already the value of the outcome. There are a few of these in the Nashville police stop dataset if we are trying to train a model that predicts when a stop results in a search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538b730",
   "metadata": {},
   "source": [
    "#### Taking care of missing values and categorical variables\n",
    "Remember above where we created a dataframe with dummy variables to encode the categorical variables `violation`, `subject_race`, and `subject_sex`? That means we have taken care of the categorical variables. We still need to do something with the NaN values for `contraband_found` since we might be interested in that. Some features, like `search_basis`, we may just want to leave as is for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34148b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode the NaN values for 'contraband_found' into something that can be summed and confirm the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1216bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_stops_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00297017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the non-numeric features from the 'cleaned_stops_dummies' df \n",
    "non_summable_cols = ...\n",
    "correlation_df = ...\n",
    "correlation_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix and a heatmap visualization for it \n",
    "# using the seaborn heatmaps library\n",
    "# the following code comes from the Seaborn examples page\n",
    "# https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = correlation_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a366a9",
   "metadata": {},
   "source": [
    "#### Notes on feature selection using the correlation matrix and heatmap\n",
    "Remember that originally `'contraband_found'` was coded as a NaN unless there had been a search; when there was a search it was coded as either True or False. We made a choice not to exclude `'contraband_found'` values with NaNs in order not to eliminate most of the sample.\n",
    "\n",
    "The problem is that now the overwhelming number of observations for `'contraband_found'` are False, since of course no contraband _could_ be found if there was no search. \n",
    "\n",
    "**Questions**\n",
    "\n",
    "1. Does this mean we can use `'contraband_found'` as a predictor for `'search_conducted'`? How about other possible predictors like `'search_person'`, `'search_vehicle'`, and `'frisk_performed'`? Why or why not?\n",
    "2. Can you find predictors from the correlation matrix that look as though they would work? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77fb81",
   "metadata": {},
   "source": [
    "**Answer:** <p>\n",
    "    _your answer here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce variables to reasonable predictors based on heatmap, no posthoc predictors\n",
    "# quick and dirty heatmap with annot=True\n",
    "# note at end about reference category and collinearity problem \n",
    "# if *all* the dummy variables are used as predictors at once\n",
    "reasonable_features = ...\n",
    "reasonable_df = correlation_df[reasonable_features]\n",
    "reasonable_matrix = reasonable_df.corr()\n",
    "plt.figure(figsize=(4,4))\n",
    "g = sns.heatmap(reasonable_matrix, annot=True)\n",
    "reasonable_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257091f",
   "metadata": {},
   "source": [
    "**One last question:**\n",
    "\n",
    "How well do you think we will be able to predict whether or not a driver was searched using the features available to us in the Nashville traffic stop data? What other data do you think would help us make better predictions?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001b52b",
   "metadata": {},
   "source": [
    "_Your answer here:_<p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3848bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_stops.search_basis.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792bac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
