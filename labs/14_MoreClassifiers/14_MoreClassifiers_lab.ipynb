{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LEGALST-123] Lab 14: More Classifiers--Decision Tree, SVM\n",
    "---\n",
    "<img src=\"https://c.pxhere.com/photos/df/5e/uganda_signs_outdoor_wooden_sign_direction_this_wy_that_way-341892.jpg!d\" style=\"width: 600px; height: 400px;\" />\n",
    "\n",
    "\n",
    "This lab used to come at the very end of the course, but it makes sense to include it a bit earlier, before we get into text, where we will explore yet more methods for classification (including predicting words). This lab will introduce more flexible methods of prediction that do not assume as much about the data they try to fit and are not bound by function form like linear models are. We will see if tree-based models or a linear support vector machine model are any better at the challenging task of predicting whether or not a traffic stop resulted in a search. \n",
    "\n",
    "In this lab we will use the 2013 stop data with the added demographic information from the ACS that we then joined using the spatial join in the previous lab. Here is how the lab will proceed. \n",
    "1. add a couple of interaction terms to the features we used in Lab 13 to try to get a better fit for the logistic regression classifier\n",
    "2. try a decision tree classifier\n",
    "3. try tree-based models (random forest and boosted trees)\n",
    "4. try a support vector machine model to separate classes \n",
    "\n",
    "Don't be discouraged if your classifiers don't work that well--it's not you, it's the data-generating process!\n",
    "\n",
    "*Estimated Time: 60 minutes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Dependencies:**\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!pip install geopandas\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import plotly.express as px\n",
    "import json\n",
    "%matplotlib inline\n",
    "!pip install scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as linear\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "# set the random seed so that results reproduce\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Nashville Police Stops \n",
    "#### (with added ACS demographic information for location of each stop)\n",
    "\n",
    "This lab will use the dataset we put together in the \"Performance Metrics\" lab, where we joined the 2013 Nashville traffic stop data with the demographic data on census block groups from the American Community Survey. Remember that this was GeoPandas data. It is a big file so be patient. We will read it into a dataframe first. Then we will have to fix the column labels, which were truncated by GeoPandas when the dataframe was written out to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shapefile from the Git repo ds-modules/data\n",
    "path = \"https://github.com/ds-modules/data/raw/main/LS123_Nashville_2013_block_groups_stop_data.shp\"\n",
    "Nashville_stops_2013geo = gpd.read_file(path)\n",
    "Nashville_stops_2013geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter on Datahub has problems with dataframes so this may not display right without restarting kernel\n",
    "Nashville_stops_2013geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nashville_stops_2013geo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to restore the column names that were truncated when the file was written out; those names are\n",
    "# in the df we made with the spatial join in Lab 13\n",
    "column_dict = {\n",
    "    'med_inc_20':\"median_income_2013\",\n",
    "    'hispanic_p':'hispanic_pct',\n",
    "    'asianpac_1':'asianpacific_pct',\n",
    "    'search_con':'search_conducted',\n",
    "    'violation_':'violation_investigative_stop',\n",
    "    'violatio_1':'violation_moving_traffic',\n",
    "    'violatio_2':'violation_vehicle_equip',\n",
    "    'subject_ag':'subject_age',\n",
    "    'subject_se':'subject_sex_male',\n",
    "    'subject_ra':'subject_race_white',\n",
    "    'subject__1':'subject_race_hispanic',\n",
    "    'subject__2':'subject_race_black',\n",
    "    'citation_i':'citation_issued'\n",
    "    }\n",
    "Nashville_stops_2013geo.rename(columns=column_dict, inplace=True)\n",
    "Nashville_stops_2013geo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Improving the logistic classifier for search\n",
    "In Lab 13 we created a logistic classifier for predicting searches after traffic stops. It did not perform all that well, and the naive prediction of \"no search\" was much more accurate. Nevertheless, we can perhaps do better than we did in Lab 13 by creating new features based on inductive theory from criminology, which is, police are even more likely to be suspicious of \"race out of place\" than they are likely to be driven by suspicion based on race. For example, police will default to more suspicion of a black motorist in a white neighborhood, or a white motorist in a black neighborhood, than they would if the motorists were \"in place\" in a neighborhood that matched the officer's construction of their race. \n",
    "\n",
    "We can operationalize this idea with interaction terms, which are the scalar products of two features. Here we can construct an interaction term `'wht_in_blk'` (for stops of white motorists) as `'wht_in_blk' = 'subject_race_white' * 'black_pct'` and do the same thing for an interaction term `'blk_in_wht'`. When you include interaction terms you also need to include their components in order to see the fixed effect of the interaction term. You do not necessarily need to use these interaction terms; the logic is that these may apply to the world of Nashville traffic stops.\n",
    "\n",
    "First, we will create a dataframe with the features we want to use in our analysis; for purposes of comparison you will probably want to use the features from Lab 13, then add the interaction terms to them. Note that we changed the column names above. After that split and scale the data as before, and then train up a logistic regression classifier for searches. We may do better, but don't get your hopes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features you used in Lab 13 making sure to include components for the interaction terms\n",
    "\n",
    "reasonable_features = [...]\n",
    "reasonable_df = Nashville_stops_2013geo[reasonable_features]\n",
    "\n",
    "# function to multiply two dataframe columns to make interaction terms; you may use another method\n",
    "# note: Pandas throws a warning\n",
    "def column_prod(ser1, ser2):\n",
    "    prods = []\n",
    "    arr1 = ...\n",
    "    arr2 = ...\n",
    "    prods = ...\n",
    "    return prods\n",
    "\n",
    "reasonable_df['wht_in_blk'] = ...\n",
    "reasonable_df['blk_in_wht'] = ...\n",
    "\n",
    "reasonable_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data as before with the training proportion as 0.6, and validation & test as 0.2 each\n",
    "# remember that you do it in two steps\n",
    "\n",
    "y = reasonable_df['search_conducted']\n",
    "X = ...\n",
    "# split the sampled data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "# split the sampled training set into training and validation sets\n",
    "X_train, X_validate, y_train, y_validate = ...\n",
    "# reminder of size of training, validation, and test sets\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_validate shape: \", X_validate.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data on the training set and apply the scaling to validation and test sets\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(...)\n",
    "X_train = ...\n",
    "X_validate = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and then fit the logistic regression model to the training set\n",
    "# then report the accuracy on the training and validation sets\n",
    "\n",
    "logit_cf = linear.LogisticRegression(penalty='l2', class_weight='balanced')\n",
    "logit_cf.fit(...)\n",
    "\n",
    "print(\"Accuracy on training set: \", ...)\n",
    "print(\"Accuracy on validation set: \", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model prediction and the probability estimate for search for each observation in validation set\n",
    "predictions = ...\n",
    "probabilities = ... # returns an array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a histogram of the probabilities of search == True the model generates, noting that \n",
    "# .predict_proba returns an array that looks like [[probability negative, probability positive]]\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[go.Histogram(x=...)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "What does the distribution of the probability of the positive class tell us about how well the classifier works? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your answer here_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's illustrate the prediction accuracy with a confusion matrix for the validation set\n",
    "\n",
    "logit_cf_matrix = ...\n",
    "logit_cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the confusion matrix as a heatmap\n",
    "\n",
    "logit_cf_cm = pd.DataFrame(...)\n",
    "logit_cf_cm = logit_cf_cm.rename(index=str, columns={0:'False', 1:'True'})\n",
    "logit_cf_cm.index = ['False', 'True']\n",
    "plt.figure(figsize = (6,4))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(logit_cf_cm, \n",
    "           annot=True,\n",
    "           fmt = '9.0f',\n",
    "           annot_kws={\"size\": 14})\n",
    "\n",
    "plt.title(\"Logistic Classifier Confusion Matrix for Search Conducted\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the logistic regression classifier's recall and precision and AUC-ROC score on the validation set\n",
    "# hint: use the sklearn 'metrics' module\n",
    "\n",
    "logit_cf_recall = ...\n",
    "logit_cf_precision = ...)\n",
    "logit_cf_auc_roc_score = ...)\n",
    "print(\"logistic regression classifer recall: \",logit_cf_recall)\n",
    "print(\"logistic regression classifer precision: \", logit_cf_precision)\n",
    "print(\"logistic regression classifer AUC-ROC score: \", logit_cf_auc_roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show which features are most predictive of search (remember that the features have been standardized so you\n",
    "# cannot interpret the coefficients as the actual increase in probability) by showing their coefficients\n",
    "\n",
    "feature_coefs = zip(list(X_train.columns),list(logit_cf.coef_[0,:]))\n",
    "list(feature_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** At this point you could also see if the logistic classifier gives different average predictions and probabilities for different population groups, as in Lab 13. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "1. Did the logistic classifier with the additional features do any better than the logistic classifier in the preceding lab? Why or why not?\n",
    "2. Is there anything interesting about the relative weights of the coefficients for the standardized features? Does it seem like there might be something to the \"race out of place\" hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your answers here_\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods\n",
    "Over the course of this class, we've seen that a single model may have significant trouble making accurate predictions. **Ensemble methods** seek to improve on the single-model method by combining the predictions from multiple base models.\n",
    "\n",
    "This lab will cover the two types of ensemble methods- averaging and boosting- using decision trees as our base model. But, one of the strengths of ensemble methods is their ability to solve many kinds of problems using many different kinds of base models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---\n",
    "## 2. Decision Trees\n",
    "\n",
    "[Decision trees](https://scikit-learn.org/stable/modules/tree.html#tree) predict target values by creating a set of decision rules and do not make as many assumptions about the data as parametric methods like linear regression, which make strong assumptions about the functional form of the prediction algorithm. Decision tree models are very flexible when it comes to fitting data, but by the same token often have the problem of overfitting to the data they are trained on.\n",
    "\n",
    "The tree is made up of *nodes*, which constitute decision points, and *branches*, which represent the outcome of the decision. Here's an example using the [Titanic](https://www.kaggle.com/c/titanic/data) data set to predict whether or not a passenger survived the sinking of the ship. Nodes are represented by the text, and branches by lines (left branch = 'yes', right branch='no').\n",
    "\n",
    "Starting at the *root node* (which in computer science, somewhat counterintuitively, is at the top), the data is split into different subgroups at each decision node going top to bottom. The very bottom nodes in the tree (the *leaves*) assign prediction values to the data. Trees can be used to predict continuous outcomes (see sklearn's [`DecisionTreeRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) but they are commonly used to predict the class of an observation. We will work with decision tree classifiers.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" style=\"width: 400px; height: 400px;\" />\n",
    "\n",
    "> *'sibsp' gives the number of siblings or spouses a passanger had on board. The left number under a leaf is the chance of survival for that subgroup; the right number is the percentage of passengers in that subgroup. *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Based on this decision tree, what would the model predict would happen to an 8-year-old boy with 2 sisters and a brother? What would the chance of survival be for a 28-year-old married man?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** The boy would be predicted to survive. The man would have survived with a 17% chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use sklearn's [`DecisionTreeClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to predict whether or not the Nashville police searched incident to a traffic stop. Keep your expectations reasonable, since here we are creating but a single tree.\n",
    "\n",
    "1. Create the `DecisionTreeClassifier()`. Set `max_depth` equal to 4.\n",
    "2. Fit `X_train` and `y_train` to the regressor to create the model\n",
    "\n",
    "\n",
    "Note: There are a large number of model parameters to set. The `criterion=` parameter measures the quality of the split at a node in the tree--gini impurity is the probability that a random draw will be misclassified. The `splitter=` parameter is the strategy for choosing the split at each node. The `max_depth=` parameter constrains how many times a data set can be split. For example, the Titanic tree had a max depth of 3 (i.e. you could pass through at most 3 branches when going from the root to a leaf). A decision tree with more layers may give terminal nodes (leaves) that are more homogeneous. Here we will set the depth to 4 (back in the day they used to be called 'ply') so that we have a tractable visualization of the resulting tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the DecisionTreeClassifier\n",
    "dt_cf = DecisionTreeClassifier(criterion='gini',  # how to measure fit\n",
    "                               splitter='best',  # or 'random' for random best split\n",
    "                               max_depth=4,  # how deep tree nodes can go\n",
    "                               min_samples_split=2,  # samples needed to split node\n",
    "                               min_samples_leaf=1,  # samples needed for a leaf\n",
    "                               min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                               max_features=None,  # number of features to look for when splitting\n",
    "                               max_leaf_nodes=None,  # max nodes\n",
    "                               random_state=10,\n",
    "                               min_impurity_decrease=1e-07)  # early stopping\n",
    "\n",
    "# fit the model\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `feature_importances_` show how much weight is given to each feature in the model. Higher numbers correspond to more important features. The importances correspond to features by their index: the importance weight in position 1 goes with feature 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the feature importance, in a data frame for convenience\n",
    "pd.DataFrame({'feature': ...,\n",
    "             'importance': ...})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the final steps:\n",
    "\n",
    "3. Check the model's accuracy on the training and validation data using `.score`\n",
    "4. Show the confusion matrix for the classifier\n",
    "5. Show the recall, precision, and AUC-ROC scores for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "print(\"accuracy on training set: \", ...)\n",
    "print(\"accuracy on validation set: \", ...)\n",
    "\n",
    "# generate predicted class and probability of search == True for the validation set\n",
    "predictions = ...\n",
    "probabilities = ...\n",
    "\n",
    "# illustrate the prediction accuracy with a confusion matrix for the validation set\n",
    "\n",
    "dtree_cf_matrix = metrics.confusion_matrix(y_validate, predictions)\n",
    "dtree_cf_cm = pd.DataFrame(dtree_cf_matrix, range(2), range(2))\n",
    "dtree_cf_cm = dtree_cf_cm.rename(index=str, columns={0:'False', 1:'True'})\n",
    "dtree_cf_cm.index = ['False', 'True']\n",
    "plt.figure(figsize = (6,4))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(dtree_cf_cm, \n",
    "           annot=True,\n",
    "           fmt = '9.0f',\n",
    "           annot_kws={\"size\": 12})\n",
    "\n",
    "plt.title(\"Decision Tree Classifier Confusion Matrix for Search Conducted\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the decision tree classifier's recall, precision, and AUC-ROC scores on the validation set\n",
    "\n",
    "dtree_cf_recall = ...\n",
    "dtree_cf_precision = ...\n",
    "dtree_cf_auc_roc_score = ...\n",
    "print(\"decision tree classifer recall: \",dtree_cf_recall)\n",
    "print(\"decision tree classifer precision: \", dtree_cf_precision)\n",
    "print(\"decision tree classifer AUC-ROC score: \", dtree_cf_auc_roc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. How did the decision tree perform relative to both the logistic regression and naive classifiers? \n",
    "2. How would you characterize the bias of the decision tree classifier versus that of the logistic regression classifier? Is there a reason to prefer one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your answer here_\n",
    "\n",
    "1. \n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "\n",
    "One of the great thing about decision trees is that, unlike many other models, it's relatively easy to visualize what is going on inside the model. The graphviz library can show the structure of the tree, as well as what decision is being made at each node.\n",
    "\n",
    "Due to some datahub limitations we can't use graphviz directly through our notebook. However, we can use the [Webgraphviz site](http://webgraphviz.com/) as a workaround. Run the cell below to generate the graphviz data for the model you just trained. Then, copy the *entire* output of the cell, click the link to Webgraphviz, replace the sample text in the text box with your copied data, and hit the button to generate the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the graphviz data\n",
    "print(export_graphviz(model, out_file = None, feature_names = X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on the visualization:\n",
    "\n",
    "- the top line of every node shows the decision that splits the data at that node\n",
    "- `samples` is the number of samples (rows) that are going through that node on the way down the tree\n",
    "- `value` is the class memberships expressed as `[negative, positive]` at that node\n",
    "- `gini` is the Gini impurity G for that node \n",
    "$$G = \\sum_{i=1}^{C} p(i) * (1-p(i))$$\n",
    "where C is the total number of classes and $p(i)$ is the probability of selecting class $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. How is the decision tree making the splits?\n",
    "2. Do you think a deeper tree would give you a better classifier? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your answers here_\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --\n",
    "## 3. Averaging Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we did not have an overfitting problem (no doubt due to the data generating process for searches and the features that are actually available to us), tree-based models usually overfit their training data. We can try to address this overfitting issue using **averaging** ensemble methods. The intuition behind averaging is to build multiple estimators, then use the average of all their predictions as the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** accomplishes this by creating multiple decision trees (a 'forest' of them, if you will), each trained on sample of data drawn at random with replacement from the given set. Additionally, when each tree is constructed, not every feature is considered as a candidate on which to split the tree for each decision point.\n",
    "\n",
    "By adding some randomization into the subsets and features that are considered by each model, then averaging the predictions across models, [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles) can typically produce a model that is better at generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Create an out-of-the-box [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) (i.e. use all the default settings for the model hyperparameters, like `n_estimators`), then fit it to the data and get the model's scores on the training and validation data. As above, also report a confusion matrix (as a dataframe) and the recall, precision, and AUC-ROC score for the Random Forest Classifier on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classifier\n",
    "rf_cf = RandomForestClassifier(n_estimators=100,  # number of trees\n",
    "                               criterion='gini',  # how to measure fit\n",
    "                               max_depth=None,  # how deep tree nodes can go\n",
    "                               min_samples_split=2,  # samples needed to split node\n",
    "                               min_samples_leaf=1,  # samples needed for a leaf\n",
    "                               min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                               max_features='sqrt',  # max feats\n",
    "                               max_leaf_nodes=None,  # max nodes\n",
    "                               n_jobs=1, # how many to run parallel\n",
    "                               random_state=10,\n",
    "                               class_weight=None) #using default even though classes unbalanced\n",
    "# fit the data \n",
    "model = ...\n",
    "#score the model on the training and validation data\n",
    "print('mean accuracy on training set: ', ...)\n",
    "print('mean accuracy on validation set: ', ...)\n",
    "\n",
    "# generate predicted class and probability of search == True for the validation set\n",
    "predictions = ...\n",
    "probabilities = ...\n",
    "\n",
    "# make confusion matrix\n",
    "rf_cf_matrix = ...\n",
    "rf_cf_cm = pd.DataFrame(rf_cf_matrix, range(2), range(2))\n",
    "rf_cf_cm = rf_cf_cm.rename(index=str, columns={0:'Pred_False', 1:'Pred_True'})\n",
    "rf_cf_cm.index = ['False', 'True']\n",
    "print(rf_cf_cm)\n",
    "\n",
    "rf_cf_recall = ...\n",
    "rf_cf_precision = ...\n",
    "rf_cf_auc_roc_score = ...\n",
    "print(\"random forest classifer recall: \",rf_cf_recall)\n",
    "print(\"random forest classifer precision: \", rf_cf_precision)\n",
    "print(\"random forest classifer AUC-ROC score: \", rf_cf_auc_roc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "1. How does Random Forest compare to Decision Tree on our metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your answer here_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "Like with most models, we can get better results by tuning the hyperparameters of the model. Let's try changing three: `max_depth`, `n_estimators`, and `min_samples_split`.\n",
    "\n",
    "#### Grid Search\n",
    "\n",
    "The process of choosing good hyperparameters can be tedious, involving a lot of trial and error. Fortunately, sklearn has a tool to help.\n",
    "\n",
    "A [**grid search**](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search) tests different possible parameter combinations to see which combination yields the best results. The grid is formatted as a dictionary, where the keys are the parameter names and the values are the different values you want to try for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter grid to look for optimal values for n_estimators, max_depth, and _min_samples_split\n",
    "param_grid = {'n_estimators': range(50, 151, 50),\n",
    "              'max_depth': range(1, 11, 5),\n",
    "              'min_samples_split': [2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the grid is made, it gets fed into a `GridSearchCV` along with the corresponding model. This may take awhile to run - the computer is calculating the score for every possible combination of parameter values in the grid and is using 5-fold [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) to keep it from overfitting. Rather than use 'accuracy' as the metric, we will use the AUC-ROC score as the summary metric of performance. Remember that our Random Forest Classifier did not fit the data well, so we may do badly here too. Fear not, since now we know that search is hard to predict with the features at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, scoring='roc_auc')\n",
    "grid_search.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fit the model to the data, information about the search process and results is stored in `.cv_results_`. Here's what you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keys stored in the grid search process results dictionary\n",
    "sorted(grid_search.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"params\" contains the different combinations of parameters that were tried. \"mean_test_score\" gives the average score (which we set to AUC-ROC) for models using each set of parameters. Items are matched by index- the ith score is for the ith set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results_df = pd.DataFrame(...)\n",
    "grid_search_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Find the set of parameters that got the best average score (`np.argmax` might help). Create a new random forest regressor using those parameters, then fit the model and print the scores for the training and validation data.\n",
    "\n",
    "**Note:** Our Random Forest Classifier performed almost the same whatever the combination of parameters. If you are modeling something where the features you have capture the variation in the data better than ours do, this exercise can be quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = ...\n",
    "best_params = grid_search.cv_results_[\"params\"][best_index]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classifier\n",
    "rf_cf_2 = RandomForestClassifier(n_estimators=...,\n",
    "                                 max_depth=...,\n",
    "                                 min_samples_split=...)\n",
    "\n",
    "# fit the model \n",
    "model = ...\n",
    "\n",
    "#score the model (this is accuracy) on the training and validation data\n",
    "print(\"second random forest classifier accuracy on training set: \", ...)\n",
    "print(\"second random forest classifier accuracy on validation set: \", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as always, there is a shortcut- if you call `.predict` or `.score` on the grid search object you originally used to find the best parameters, it will do so using the best set of parameters automatically. Note that the score might be slightly different from the one in the model you just calculated due to the 'random' part of 'random forest'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC-ROC for model with best parameters: \", grid_search.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting Methods <a id='section 3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting** algorithms work roughly like so:\n",
    "1. Make a weak predictor (one that makes predictions with slightly better-than-chance accuracy)\n",
    "2. Train and evaluate the weak predictor\n",
    "3. Make a new weak predictor that takes into account the errors made in the last model and improves on them.\n",
    "4. Repeat steps 2 and 3 many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ada Boost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) (for ADAptive BOOSTing) is one of the most popular boosting algorithms. The adaptive part of the algorithm comes from how it updates the data for each weak model in the sequence.\n",
    "\n",
    "Each sample $i$ in the training set is weighted by some number $w_i$, and the input to the model is the samples multiplied by the weights. At first, all the $w_i$s are the same. After the first model is evaluated, the weights are updated so that samples that were predicted incorrectly get higher weights and samples that were predicted correctly get lower weights.\n",
    "\n",
    "**QUESTION:** In the playground game \"Duck, Duck, Goose\", children are labeled as 'ducks' or 'geese' in the name of schoolyard mayhem. Suppose we want to build a classifier that predicts whether a sample is a duck or a goose based on two features: color, and whether or not it quacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = pd.DataFrame({'color':['white', 'grey', 'white'],\n",
    "                      'quacks':['yes', 'yes', 'no'],\n",
    "                     'type':['duck', 'duck', 'goose']})\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, all the weights are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds['weights'] = np.ones(3) / 3\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model in a sequence for Ada Boost outputs the following predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds['predictions'] = ['duck', 'goose', 'goose']\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For samples 0, 1 and 2, state whether their corresponding weight would be adjusted higher, lower, or stay the same before the data is fed into the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "- Sample 0: \n",
    "- Sample 1: \n",
    "- Sample 2: \n",
    "\n",
    "Using an Ada Boost Classifier is similar to using the other tree-based classifiers above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the classifier\n",
    "ada_cf = AdaBoostClassifier(base_estimator=None,  # default is decision tree \n",
    "                            n_estimators=50,  # number of trees to try before stopping\n",
    "                            learning_rate=1.0,  # decrease influence of each additional estimator\n",
    "                            random_state=10) # sets the random seed\n",
    "                            \n",
    "# fit the data\n",
    "model = ...\n",
    "\n",
    "# get the model predictions and the probabilities of predicting positive class and metrics\n",
    "predictions = ...\n",
    "probabilities = ...\n",
    "ada_cf_recall = ...\n",
    "ada_cf_precision = ...\n",
    "ada_cf_auc_roc_score = ...\n",
    "\n",
    "# report metrics\n",
    "print(\"AdaBoost Model accuracy on training set: \", ...)\n",
    "print(\"AdaBoost Model accuracy on validation set: \", ...)\n",
    "print(\"AdaBoost classifer recall: \", ada_cf_recall)\n",
    "print(\"AdaBoost classifer precision: \", ada_cf_precision)\n",
    "print(\"AdaBoost classifer AUC-ROC score: \", ada_cf_auc_roc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You could use grid search as we did above to find better values for the Ada Boost parameters. See the AdaBoost Classifier [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for more details). \n",
    "\n",
    "#### Question\n",
    "How does the AdaBoost classifier do compared to the other classifiers? What have we learned about the search process, at least with respect to search in Nashville traffic stops?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your answer here_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In summary:\n",
    "- decision trees make predictions using 'if/then/else' rules to split data into subsets, but are highly subject to overfitting\n",
    "- grid search can be a useful tool for tuning hyperparameters\n",
    "- ensemble methods seek to improve models by averaging or boosting multiple models\n",
    "- random forest uses randomness and averaging to counter the overfitting problem for decision trees\n",
    "- Ada boost can be used on a variety of model types to taking an initial weak model and improve it with sequential boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---\n",
    "## 4. Support Vector Classifier\n",
    "There is one other type of supervised learning method that is also used for classification--[support vector machines](https://scikit-learn.org/stable/modules/svm.html). The basic intuition is that we want to find an $n$-dimensional hyperplane (defined by an equation of the form, for $i$ from 1 to $n$ dimensions)\n",
    "$$\\beta_0 + \\beta_1x_i1 + \\beta_2x_i2 + ... + \\beta_px_ip = 0$$\n",
    "that separates the classes in our data. When the hyperplane can completely separate the data, we have a maximal marginal classifier, which must satisfy the constraints that all of the members of each class are on the appropriate side of the hyperplane and that the margin between the closest points to the boundary and the boundary is maximized. See Chapter 9 in _An Introduction to Statistical Learning_ for a thorough explanation. \n",
    "\n",
    "The support vector classifier is a more flexible version of the maximal marginal classifier that relaxes the requirement that all the points in each class be on the correct side of the margin, or even the correct side of they hyperplane. This allows support vector classifiers to misclassify some observations in return for doing a better job of classifying most of the training observations. \n",
    "\n",
    "The decision rule of support vector classifiers relies only on the subset of training observations which, if they moved, would move the separating hyperplane (the vectors originating at those training observations and perpendicular to the separating hyperplane are the support vectors). Support vector classifiers can work in high-dimensional spaces (text!) but use only the subset of the training observations and so don't use up so much memory. Here the intent is just to try it out -- we already suspect that the Nashville police stops are not readily separable into true and false classes when it comes to `search_conducted`. Note that support vector machines do not provide probability estimates, and that sometimes the optimization algorithm that attempts to locate the hyperplane for maximum separation of classes fails to converge on a solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a linear support vector classifier\n",
    "# Note: C is a tuning parameter--the larger C is, the more violations of being \n",
    "# on the correct side of margin are allowed and the more the bias-variance tradeoff is shifted toward higher\n",
    "# bias and lower variance\n",
    "\n",
    "svcf = svm.LinearSVC(C=10, class_weight='balanced', verbose=1, max_iter=1000, random_state=10)\n",
    "\n",
    "# fit the Linear SVC to the training data \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get model predictions for validation set\n",
    "predictions = ...\n",
    "svc_recall = ...\n",
    "svc_precision = ...\n",
    "\n",
    "# get a read on its accuracy on the validation set\n",
    "print(\"support vector classifier accuracy: \", ...)\n",
    "print(\"support vector classifier recall: \", svc_recall)\n",
    "print(\"suport vector classifier precision: \", svc_precision)\n",
    "\n",
    "# show a df with the features and their coefficients in the model\n",
    "svcf_features = pd.DataFrame({'feature':svcf.feature_names_in_, 'coefficient':svcf.coef_[0,:]})\n",
    "svcf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Which classifier performed the best? Was the best performance good enough to allow us to predict search outcomes from a traffic stop?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "- Random Forest, ADA Boost, grid search code adapted from https://github.com/dlab-berkeley/python-machine-learning/blob/master\n",
    "- Ensemble methods general reference: http://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto (Spring 2018) and amended by Jonathan Marshall (Spring 2024)\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
