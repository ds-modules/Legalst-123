{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LEGALST-123] Lab 14: Other Prediction Algorithms & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this lab, we will spend some time looking into different classification algorithms from the ones you have seen in previous labs so far. To aid us in our prediction tasks, we will also be learning about how to effectively use feature selection to make sure we are optimizing our use of the data. For this purpose we'll be utilizing the Nashville police stops dataset that you have seen in several labs thus far.\n",
    "\n",
    "- **Other Prediction Algorithms**: \n",
    "    - We will cover a handful of other classification algorithms, including support vector machines (SVMs), decision trees, random forests, and naive Bayes. For each algorithm, we'll provide some intuition behind which circumstances they are most useful for and how to judge their performance.\n",
    "\n",
    "- **Feature Selection**:\n",
    "    - We will learn about the process of feature selection, which will allow us to optimize our model and reduce overfitting of our model, which we learned about in the previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Prediction Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, we'll be using the Nashville police stops dataset again, but we'll be using a cleaned sample of it that we obtained in a previous lab. This sample contains 833 rows and 30 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>location</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>precinct</th>\n",
       "      <th>reporting_area</th>\n",
       "      <th>zone</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>...</th>\n",
       "      <th>contraband_drugs</th>\n",
       "      <th>contraband_weapons</th>\n",
       "      <th>frisk_performed</th>\n",
       "      <th>search_conducted</th>\n",
       "      <th>search_person</th>\n",
       "      <th>search_vehicle</th>\n",
       "      <th>search_basis</th>\n",
       "      <th>reason_for_stop</th>\n",
       "      <th>vehicle_registration_state</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1840907</td>\n",
       "      <td>2010-04-18</td>\n",
       "      <td>13140.0</td>\n",
       "      <td>BURGESS AVE &amp; WHITE BRIDGE PIKE, NASHVILLE, TN...</td>\n",
       "      <td>36.145004</td>\n",
       "      <td>-86.857970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5103.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>TN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492044</td>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>19920.0</td>\n",
       "      <td>DUE WEST AVE W &amp; S GRAYCROFT AVE, MADISON, TN,...</td>\n",
       "      <td>36.249187</td>\n",
       "      <td>-86.734459</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1797.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vehicle equipment violation</td>\n",
       "      <td>TN</td>\n",
       "      <td>tail light out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>431170</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>S GALLATIN PIKE &amp; MADISON BLVD, MADISON, TN, 3...</td>\n",
       "      <td>36.254979</td>\n",
       "      <td>-86.715246</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>TN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2066423</td>\n",
       "      <td>2013-05-17</td>\n",
       "      <td>62760.0</td>\n",
       "      <td>CHARLOTTE PIKE &amp; W HILLWOOD DR, NASHVILLE, TN,...</td>\n",
       "      <td>36.139093</td>\n",
       "      <td>-86.880533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5009.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vehicle equipment violation</td>\n",
       "      <td>TN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2899480</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>28140.0</td>\n",
       "      <td>BELL RD &amp; DODSON CHAPEL RD, HERMITAGE, TN, 37076</td>\n",
       "      <td>36.163310</td>\n",
       "      <td>-86.613147</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9501.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moving traffic violation</td>\n",
       "      <td>TN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        date     time  \\\n",
       "0  1840907  2010-04-18  13140.0   \n",
       "1   492044  2015-01-19  19920.0   \n",
       "2   431170  2015-01-15   1020.0   \n",
       "3  2066423  2013-05-17  62760.0   \n",
       "4  2899480  2010-09-01  28140.0   \n",
       "\n",
       "                                            location        lat        lng  \\\n",
       "0  BURGESS AVE & WHITE BRIDGE PIKE, NASHVILLE, TN...  36.145004 -86.857970   \n",
       "1  DUE WEST AVE W & S GRAYCROFT AVE, MADISON, TN,...  36.249187 -86.734459   \n",
       "2  S GALLATIN PIKE & MADISON BLVD, MADISON, TN, 3...  36.254979 -86.715246   \n",
       "3  CHARLOTTE PIKE & W HILLWOOD DR, NASHVILLE, TN,...  36.139093 -86.880533   \n",
       "4   BELL RD & DODSON CHAPEL RD, HERMITAGE, TN, 37076  36.163310 -86.613147   \n",
       "\n",
       "   precinct  reporting_area   zone  subject_age  ... contraband_drugs  \\\n",
       "0       1.0          5103.0  113.0         23.0  ...              NaN   \n",
       "1       7.0          1797.0  723.0         45.0  ...              NaN   \n",
       "2       7.0          1623.0  711.0         21.0  ...              NaN   \n",
       "3       1.0          5009.0  123.0         35.0  ...              NaN   \n",
       "4       5.0          9501.0  521.0         53.0  ...              NaN   \n",
       "\n",
       "  contraband_weapons frisk_performed search_conducted search_person  \\\n",
       "0                NaN           False            False         False   \n",
       "1                NaN           False            False         False   \n",
       "2                NaN           False            False         False   \n",
       "3                NaN           False            False         False   \n",
       "4                NaN           False            False         False   \n",
       "\n",
       "   search_vehicle  search_basis              reason_for_stop  \\\n",
       "0           False           NaN     moving traffic violation   \n",
       "1           False           NaN  vehicle equipment violation   \n",
       "2           False           NaN     moving traffic violation   \n",
       "3           False           NaN  vehicle equipment violation   \n",
       "4           False           NaN     moving traffic violation   \n",
       "\n",
       "  vehicle_registration_state           notes  \n",
       "0                         TN             NaN  \n",
       "1                         TN  tail light out  \n",
       "2                         TN             NaN  \n",
       "3                         TN             NaN  \n",
       "4                         TN             NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = pd.read_csv(\"https://github.com/ds-modules/data/raw/main/nashville_cleaned.csv\", index_col=0).reset_index()\n",
    "stops.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start getting into some of our prediction tasks, we'll drop the columns with many null values that are irrelevant for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.drop(labels = ['contraband_found', 'contraband_drugs', 'contraband_weapons', 'search_basis', 'notes'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to decide which feature of our data we would like to predict. Because we'll be practicing with more classifcation algorithms today, we want to choose a column that can easily be split into two or more classes, if it's not already presented like that. Let's take a look at our columns again below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 833 entries, 0 to 832\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   index                       833 non-null    int64  \n",
      " 1   date                        833 non-null    object \n",
      " 2   time                        833 non-null    float64\n",
      " 3   location                    833 non-null    object \n",
      " 4   lat                         833 non-null    float64\n",
      " 5   lng                         833 non-null    float64\n",
      " 6   precinct                    833 non-null    float64\n",
      " 7   reporting_area              833 non-null    float64\n",
      " 8   zone                        833 non-null    float64\n",
      " 9   subject_age                 833 non-null    float64\n",
      " 10  subject_race                833 non-null    object \n",
      " 11  subject_sex                 833 non-null    object \n",
      " 12  officer_id_hash             833 non-null    object \n",
      " 13  type                        833 non-null    object \n",
      " 14  violation                   833 non-null    object \n",
      " 15  arrest_made                 833 non-null    bool   \n",
      " 16  citation_issued             833 non-null    bool   \n",
      " 17  warning_issued              833 non-null    bool   \n",
      " 18  outcome                     833 non-null    object \n",
      " 19  frisk_performed             833 non-null    bool   \n",
      " 20  search_conducted            833 non-null    bool   \n",
      " 21  search_person               833 non-null    bool   \n",
      " 22  search_vehicle              833 non-null    bool   \n",
      " 23  reason_for_stop             833 non-null    object \n",
      " 24  vehicle_registration_state  833 non-null    object \n",
      "dtypes: bool(7), float64(7), int64(1), object(10)\n",
      "memory usage: 123.0+ KB\n"
     ]
    }
   ],
   "source": [
    "stops.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this information, we can see that some of the columns such as `arrest_made` or `citation_issued` are presented as `True` or `False` values. These may be good options for our classification tasks, as they could also present us with some interesting algorithms: based on the features of the individual row, can we accurately predict whether an arrest was made or a citation was issued? \n",
    "\n",
    "Let's take a look at the distribution of `True/False` values for these columns below with the `.value_counts()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    816\n",
       "True      17\n",
       "Name: arrest_made, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops['arrest_made'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    660\n",
       "True     173\n",
       "Name: citation_issued, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops['citation_issued'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     684\n",
       "False    149\n",
       "Name: warning_issued, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops['warning_issued'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among these three columns, it looks like the `citation_issued` column has the more well-distributed split between the true and false values. Our prediction algorithms will typically do better when the split between the two classes is more even, so we'll use that column for the binary prediction tasks below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression lab, we discussed the importance of doing train-test splits on our data, so in the cell below we'll perform this split and then define our 2-D DataFrame of features in a variable `X_binary` (for now, we'll use all of the numerical columns in the DataFrame as features for our prediction), our 1-D array of labels in another variable `y_binary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features used to predict citation issuing status\n",
    "X_binary = stops[['time', 'lat', 'lng', 'precinct', 'reporting_area', 'zone', 'subject_age']]\n",
    "\n",
    "# whether a citation was issued or not\n",
    "y_binary = stops['citation_issued']\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data with 0.2 proportion for test size\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test \n",
    "X_binary_train, X_binary_test, y_binary_train, y_binary_test = train_test_split(X_binary, y_binary, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE COMPLETED: SVM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees predict target values by creating a set of decision rules. The tree is made up of *nodes*, which constitute decision points, and *branches*, which represent the outcome of the decision. Here's an example using the [Titanic](https://www.kaggle.com/c/titanic/data) data set to predict whether or not a passenger survived the sinking of the ship. Nodes are represented by the text, and branches by lines (left branch = 'yes', right branch = 'no').\n",
    "\n",
    "Starting at the *root node* (which in computer science, somewhat counterintuitively, is at the top), the data is split into different subgroups at each decision node going top to bottom. The very bottom nodes in the tree (the *leaves*) assign prediction values to the data. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" style=\"width: 400px; height: 400px;\" />\n",
    "\n",
    "> 'sibsp' gives the number of siblings or spouses a passanger had on board. The left number under a leaf is the chance of survival for that subgroup; the right number is the percentage of passengers in that subgroup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Based on the decision tree above, what would the model predict would happen to an 8-year-old boy with 2 sisters and a brother? What would the chance of survival be for a 28-year-old married man?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# The boy would be predicted to survive. The man would have survived with a 17% chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is very similar to the other scikit-learn models you've used.\n",
    "1. Create the `DecisionTreeRegressor()`. Set `max_depth` equal to 3.\n",
    "2. Fit `X_train` and `y_train` to the regressor to create the model\n",
    "\n",
    "\n",
    "Note: The `max_depth =` parameter of DecisionTreeRegression constrains how many times a data set can be split. For example, the Titanic tree had a max depth of 3 (i.e. you could pass through at most 3 branches when going from the root to a leaf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we perform the two-step process listed above. You'll notice that we decide to utilize a lot of the optional parameters listed in the DecisionTree method! If we didn't want to use so many of the parameters, we simply could do `dt_reg = DecisionTreeRegressor()`, and it would also work in creating a DecisionTree model -- it would just be less specific than the one we've created below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the DecisionTreeRegressor\n",
    "dt_reg = DecisionTreeRegressor(criterion = 'squared_error',  # how to measure fit\n",
    "                               splitter = 'best',  # or 'random' for random best split\n",
    "                               max_depth = 3,  # how deep tree nodes can go\n",
    "                               min_samples_split = 2,  # samples needed to split node\n",
    "                               min_samples_leaf = 1,  # samples needed for a leaf\n",
    "                               min_weight_fraction_leaf = 0.0,  # weight of samples needed for a node\n",
    "                               max_features = None,  # number of features to look for when splitting\n",
    "                               max_leaf_nodes = None,  # max nodes\n",
    "                               min_impurity_decrease = 1e-07)  # early stopping\n",
    "\n",
    "# fit the model\n",
    "dt_model = dt_reg.fit(X_binary_train, y_binary_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model created and fitted, we can use the `.score()` method to test it out on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.151622010144\n",
      "-0.220985663765\n"
     ]
    }
   ],
   "source": [
    "# score the model\n",
    "print(dt_model.score(X_binary_train, y_binary_train))\n",
    "print(dt_model.score(X_binary_test, y_binary_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we can receive with this method is 1.0 -- scores very close to zero and negative scores imply the model does not perform very well. This model was just a base model that utilized all numerical features from our dataset (regardless of whether they might be good estimators for citation status or not). As we explore more models in this lab and get into feature importance and selection, we can get a better understanding of why the decision tree doesn't perform very well in this case and see what we can do to improve it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one benefit of decision trees that we saw above with the Titanic variable is that they are very interpretable and easy to visualize. We can use the `export_graphviz` function below to get a better understanding of the splits that constitute our decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph Tree {\n",
      "node [shape=box, fontname=\"helvetica\"] ;\n",
      "edge [fontname=\"helvetica\"] ;\n",
      "0 [label=\"reporting_area <= 9403.0\\nsquared_error = 0.165\\nsamples = 666\\nvalue = 0.209\"] ;\n",
      "1 [label=\"subject_age <= 52.5\\nsquared_error = 0.149\\nsamples = 614\\nvalue = 0.182\"] ;\n",
      "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
      "2 [label=\"time <= 65280.0\\nsquared_error = 0.167\\nsamples = 522\\nvalue = 0.213\"] ;\n",
      "1 -> 2 ;\n",
      "3 [label=\"squared_error = 0.192\\nsamples = 367\\nvalue = 0.259\"] ;\n",
      "2 -> 3 ;\n",
      "4 [label=\"squared_error = 0.093\\nsamples = 155\\nvalue = 0.103\"] ;\n",
      "2 -> 4 ;\n",
      "5 [label=\"reporting_area <= 9296.0\\nsquared_error = 0.011\\nsamples = 92\\nvalue = 0.011\"] ;\n",
      "1 -> 5 ;\n",
      "6 [label=\"squared_error = 0.0\\nsamples = 91\\nvalue = 0.0\"] ;\n",
      "5 -> 6 ;\n",
      "7 [label=\"squared_error = 0.0\\nsamples = 1\\nvalue = 1.0\"] ;\n",
      "5 -> 7 ;\n",
      "8 [label=\"lat <= 36.216\\nsquared_error = 0.25\\nsamples = 52\\nvalue = 0.519\"] ;\n",
      "0 -> 8 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
      "9 [label=\"time <= 12570.0\\nsquared_error = 0.23\\nsamples = 39\\nvalue = 0.641\"] ;\n",
      "8 -> 9 ;\n",
      "10 [label=\"squared_error = 0.0\\nsamples = 3\\nvalue = 0.0\"] ;\n",
      "9 -> 10 ;\n",
      "11 [label=\"squared_error = 0.212\\nsamples = 36\\nvalue = 0.694\"] ;\n",
      "9 -> 11 ;\n",
      "12 [label=\"lng <= -86.622\\nsquared_error = 0.13\\nsamples = 13\\nvalue = 0.154\"] ;\n",
      "8 -> 12 ;\n",
      "13 [label=\"squared_error = 0.076\\nsamples = 12\\nvalue = 0.083\"] ;\n",
      "12 -> 13 ;\n",
      "14 [label=\"squared_error = 0.0\\nsamples = 1\\nvalue = 1.0\"] ;\n",
      "12 -> 14 ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(export_graphviz(dt_model, out_file = None, feature_names = X_binary_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Random Forest** is an example of an **ensemble method**, a prediction algorithm that we'll cover in a future lab. Essentially, ensemble methods work by building multiple estimators and then using the average (or majority) prediction as the final one. Random Forests do this by by creating multiple decision trees (a 'forest' of them, if you will), each trained on sample of data drawn at random with replacement from the given set. Additionally, when each tree is constructed, not every feature is considered as a candidate on which to split the tree for each decision point.\n",
    "\n",
    "By adding some randomization into the subsets and features that are considered by each model, then averaging the predictions across models, Random Forest can typically produce a model that is better at generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Random Forest model using scikit-learn, we can utilize the `RandomForestRegressor()` method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** In the cell below, create a base `RandomForestRegressor()` model (i.e. don't worry about passing in any hyperparameters!) and fit it to the training data. Then, use the `score()` method on our training and testing sets to see how the model performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853639651618\n",
      "-0.206872291022\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# create the RandomForestRegressor model\n",
    "# rf_reg = ...\n",
    "\n",
    "# fit the model to our training data\n",
    "# rf_model = ...\n",
    "\n",
    "# print the scores of the model on training and testing data\n",
    "# print(...)\n",
    "# print(...)\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "# create the RandomForestRegressor model\n",
    "rf_reg = RandomForestRegressor()\n",
    "\n",
    "# fit the model to our training data\n",
    "rf_model = rf_reg.fit(X_binary_train, y_binary_train)\n",
    "\n",
    "# print the scores of the model on training and testing data\n",
    "print(rf_model.score(X_binary_train, y_binary_train))\n",
    "print(rf_model.score(X_binary_test, y_binary_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely noticed that this model performed well on our training set, but just as badly as the decision trees on our testing set! As before, we'll see in the second part of this lab how to utilize feature engineering properly and pick the best features to improve these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, because our Random Forest model picks the average prediction of many different decision tree models, it's not as easy to interpret or visualize our model! However, one of the benefits of random forests is that they typically perform better on our test set and reduce the amount of overfitting to our training set than decision trees do. \n",
    "\n",
    "Thus, when deciding which model you would like to use for your prediction tasks, **it's important to consider how important interpretability is for your situation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE COMPLETED: Naive Bayes predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started with the process of feature selection, we'll use the process of **one-hot encoding** as a sort of feature engineering for our categorical variables. For example, features such as `subject_race`, `subject_sex`, and `violation` could end up being useful features for our classification models, but we currently can't use them because they're not in a numerical format!\n",
    "\n",
    "To do this, we can use a helpful function from `pandas` called `pd.get_dummies` to obtain one-hot-encoded (also known as \"dummy\") variables for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'date', 'time', 'location', 'lat', 'lng', 'precinct',\n",
       "       'reporting_area', 'zone', 'subject_age', 'officer_id_hash', 'type',\n",
       "       'arrest_made', 'citation_issued', 'warning_issued', 'outcome',\n",
       "       'frisk_performed', 'search_conducted', 'search_person',\n",
       "       'search_vehicle', 'reason_for_stop', 'vehicle_registration_state',\n",
       "       'subject_sex_female', 'subject_sex_male',\n",
       "       'subject_race_asian/pacific islander', 'subject_race_black',\n",
       "       'subject_race_hispanic', 'subject_race_other', 'subject_race_unknown',\n",
       "       'subject_race_white', 'violation_investigative stop',\n",
       "       'violation_moving traffic violation', 'violation_parking violation',\n",
       "       'violation_registration', 'violation_safety violation',\n",
       "       'violation_seatbelt violation',\n",
       "       'violation_vehicle equipment violation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops_w_dummies = pd.get_dummies(data = stops, columns = ['subject_sex', 'subject_race', 'violation'])\n",
    "stops_w_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at our columns for this new DataFrame with our dummy variables, we see that there is a column for each possible value within the categorical variables that we one-hot-encoded. To see what these columns look like, let's take a look at the `\"subject_sex_male\"` column below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subject_sex_male</th>\n",
       "      <th>citation_issued</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1840907</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492044</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>431170</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2066423</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2899480</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>2475346</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>211168</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>2697517</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>2782384</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>2907673</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>833 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  subject_sex_male  citation_issued\n",
       "0    1840907                 1            False\n",
       "1     492044                 0            False\n",
       "2     431170                 0            False\n",
       "3    2066423                 1            False\n",
       "4    2899480                 0            False\n",
       "..       ...               ...              ...\n",
       "828  2475346                 0            False\n",
       "829   211168                 0             True\n",
       "830  2697517                 0            False\n",
       "831  2782384                 1            False\n",
       "832  2907673                 1            False\n",
       "\n",
       "[833 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops_w_dummies[['index', 'subject_sex_male', 'citation_issued']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of having one column that incodes the subject's sex as a category, we have two different columns (`\"subject_sex_male\"` and `\"subject_sex_female\"`) that contain either 1's or 0's, depending on if the subject was male or not. We can now use these different categorical variables in our model creation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a new copy of the DataFrame that contains only numerical or one-hot-encoded columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_w_dummies.drop(['date', 'location', 'officer_id_hash', 'type', 'outcome', 'reason_for_stop', 'vehicle_registration_state'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: need to find some way of using .corr() or creating heatmap, but not possible with this many columns currently. may need to reevaluate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: after looking into feature importance / correlations, can then copy over a lot of the methods from old lab 14 (feature selection lab). just needs some reframing for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning & Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats! You've completed Lab 14.\n",
    "\n",
    "Hopefully, you now understand how to create different types of classification models and fit them to your data, as well as the different types of feature selection and hyperparameter tuning we can perform to improve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
