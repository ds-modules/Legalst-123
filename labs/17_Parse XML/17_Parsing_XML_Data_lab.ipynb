{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyK_ohUkPraG"
   },
   "source": [
    "# [LEGALST-123] Lab 17: Parsing XML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwWTi_M5PraK"
   },
   "source": [
    "This lab will cover parsing XML and attribute lookup, Beautiful Soup, and web scraping.\n",
    "\n",
    "*Estimated Time: 45 Minutes *\n",
    "\n",
    "### Topics Covered:\n",
    "- XML syntax\n",
    "- locating content with Beautiful Soup\n",
    "- Web scraping\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[The Data](#section-data)<br>\n",
    "1 - [Web Scraping](#section-1)<br>\n",
    "2 - [XML Syntax](#section-2)<br>\n",
    "3 - [Using Beautiful Soup to parse XML](#section-3)<br>\n",
    "4 - [Putting it all in a dataframe](#section-4)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.cElementTree as ET #XML Parser\n",
    "from lxml import etree #ElementTree and lxml allow us to parse the XML file.\n",
    "import requests #make request to server\n",
    "import time #pause loop\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SUNENg-PraU"
   },
   "source": [
    "----\n",
    "## The Data<a id='section-data'></a>\n",
    "\n",
    "In this notebook, you'll be working with XML files from the Old Bailey API (https://www.oldbaileyonline.org/obapi/). These files contain the proceedings of all trials from 1674 to 1913. For this lab, we'll go through the trials from 1754-1756. XML (eXtensible Markup Language) provides a hierarchical representation of data contained within different tags and nodes. We'll go over XML syntax later. We will learn how to parse through these XML files from Old Bailey and grab information from sections of an XML file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRiFQ0xNPrbh"
   },
   "source": [
    "----\n",
    "## Section 1: Web Scraping<a id='section-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maI4gqRPPrbi"
   },
   "source": [
    "First we will go through how to parse one XML file. The Old Bailey API has a total of **197751** cases. Fortunately, we are only going to use the ones from 1754-1756, but that still only narrows the number of cases to somewhere above 1300! \n",
    "\n",
    "Don't worry though, you're not going to manually download each case yourself. This is where web scraping comes into play. With web scraping, we can automate data collection to get all the cases. \n",
    "\n",
    "Before we start scraping, we need to know how `requests` works. The `requests` library gets (`.get`!) you a response object from a web server and will automatically decode the content from the server, from which you can use `.json()` to see the document! Requests through the Old Bailey API will return a dictionary, embedded in which is the XML representation of the trial account, which we can then write as a file and save.\n",
    "\n",
    "Let's take a look at all of the terms we can use to choose the specific cases we want. We use `.json()` here since the parameters are stored as a JSON object.\n",
    "\n",
    "<span style=\"color:red\">**Note**: The Old Bailey Online website has changed quite a lot and so we will be using the archived site to get this JSON object. The archived site will disappear in September 2024.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 569,
     "status": "error",
     "timestamp": 1584242564840,
     "user": {
      "displayName": "Violet Yao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFt1BCGd8JlHSVKhi8kVxouO0J-pkDwzodp18k6w=s64",
      "userId": "10253555746784075040"
     },
     "user_tz": 420
    },
    "id": "FzJc-Z-dPrbi",
    "outputId": "5103194b-863c-4c0a-b752-4a58c4ef2d4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rjson = requests.get('https://www.dhi.ac.uk/oldbaileyonline/obapi/terms').json()\n",
    "rjson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM9zTvLNPrbk"
   },
   "source": [
    "As I noted before, the [Old Bailey Online](https://www.oldbaileyonline.org/) website has [changed](https://www.oldbaileyonline.org/about/whats-new) quite a bit. The changes are not just to the front end but throughout the site, including the [API](https://www.oldbaileyonline.org/about/api). For example, the API now returns just ten trial account files at a time, and they are no longer just the XML object. The query parameters, like `fromdate_` and `todate_` have also changed, so this lab has had to be revised.\n",
    "\n",
    "Now that you've had a chance to look through some of the terms, let's see how to grab the specific trial account files.\n",
    "\n",
    "Clicking the URL below returns a web search result of the total number of cases, and a listing of all the cases, containing the term \"sheffield\" and the offence categrory \"deception\" from June 14th, 1847 onward. Also, each trial ID that satisfies the terms is returned; the count parameter in this case returns 74 trial IDs. The API returns how many trial IDs it finds, but it only hands out the trial account files ten at a time. The query parameter for start year is `year_gte=` and for end year is `year_lte=`\n",
    "\n",
    "https://www.dhi.ac.uk/api/data/oldbailey_record?month_gte=6&offence=deception&text=Sheffield&year_gte=1847#results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zp7TXx3WPrbl"
   },
   "source": [
    "**Question 1.1:** Use requests.get(...) to get all the trials between the years 1754 and 1756 and return them as a JSON object, and then find how many total trial accounts there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9kOCZ2HPrbn"
   },
   "outputs": [],
   "source": [
    "trials = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it might pay to look at the JSON object that `requests.get` returned to see what the list of trial accounts actually looks like. You can see that the first `hits` is a dictionary index which records a `total` for the query, and that the second `hits` is the key for the list of trial accounts.\n",
    "\n",
    "You can see below that trials is a big dictionary, inside of which is a list of trials, each of which has a trial ID. The API documentation talks about what [data endpoints](https://www.oldbaileyonline.org/about/api#data_endpoints) and [query parameters](https://www.oldbaileyonline.org/about/api#supported_parameters) you can use. Unfortunately, the data now come in a format that is more complicated that the earlier format, so we will have to do some work to extract all the trials we want over the two year period (1754-1756) that we want to look at. The `total:` key below tells us that there are a total of 1312 trial records in the period 1754-1756. The numbering scheme for the `idkey:` also tells us something about the organization of the records; the prefix `f` is for front matter, while `t` is for trial account. After that is the date with digits for year, month, and day, followed by a hyphen with how many accounts are in the series for that day. Note that many trials occurred on a day when the Old Bailey was in session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9kOCZ2HPrbn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_hits = ...\n",
    "total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the JSON object is pretty complicated. There is a top-level dictionary with the key 'hits' and then an embedded dictionary with the key 'total' (that is where we got the total number of trial accounts above) and then a second 'hits' and then a list of dictionaries and more embedded dictionaries. This is considerably more complicated than the old representation. Notice also that the very first XML document is not a trial record but is instead the front matter from that printed version of the Old Bailey Proceedings.\n",
    "\n",
    "So with the new API it looks like we may have to walk through each group of ten trial account documents until we have gotten the entire list, and then eventually parse the XML tree for each document using Beautiful Soup. For the problem set we will use the XML [dataset archived](https://www.oldbaileyonline.org/about/data#toc3) at the University of Sheffield so we will not have to scrape them from the site. Even the zipped file of Sessions Papers is very large (335MB) so you will have to plan for where you can do the work.\n",
    "\n",
    "In the JSON object that we got back, [object.hits.hits](https://www.oldbaileyonline.org/about/api#response_format) is the list of the XML docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trials['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively call requests.get for the years we want until we have a list of all the XMLs of trials\n",
    "# start_yr = 1754\n",
    "# end_yr = 1756\n",
    "\n",
    "# the nested loops are cumbersome but I am not sure of any other way of producing a list of trial contents in one step\n",
    "i = 0\n",
    "xml_list = []\n",
    "while i < total_hits:\n",
    "    trials = requests.get(f'https://www.dhi.ac.uk/api/data/oldbailey_record?from={i}&year_gte=1754&year_lte=1756').json()\n",
    "    ten_xmls = ...\n",
    "    j=0\n",
    "    while j < len(ten_xmls):\n",
    "        one_xml = ...\n",
    "        xml_list...\n",
    "        j = j + 1\n",
    "    i = i + 10\n",
    "    time.sleep(0.1)    # avoid overloading server\n",
    "len(xml_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first trial record in the list\n",
    "xml_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another\n",
    "xml_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see clearly what the requests call got us. It is a list of dictionaries that has various keys, including 'idkey' for the document ID, 'images' for the page images of the paper Old Bailey Proceeds, and 'text', which shows **just the plain text** for the document. We did not get the XML representation of the trial record, which has tags and labels for the parties etc., as we saw when we got the list of terms above. \n",
    "\n",
    "What we need to do next is\n",
    "* get the list of ID keys for all 1312 documents from 1754 to 1756 (some of these will not be trial session records but will be front matter or advertising--we won't worry about that now)\n",
    "* use the list of ID keys to make requests for the xml files for each ID key\n",
    "* write the xml files to the local data directory\n",
    "* use those xml files to build a dataframe\n",
    "\n",
    "First we need to pick out the document ID from each item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_list[5]..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9paTSQLwPrbo"
   },
   "source": [
    "Now, let's get a list of document IDs we can work with. Then we can use the first ten to demonstrate the next step of calling the API to get the xml file for each document ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Tkvn-R7Prbq"
   },
   "outputs": [],
   "source": [
    "doc_ids = [...]\n",
    "first_10 = doc_ids[:10]\n",
    "first_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puWGLo37Prbs"
   },
   "source": [
    "Using the trial IDs from the previous cell, we are going to format the URL in a way so that we can get the XML file for each trial. In order to get the XML file using the Old Bailey API, we must follow this URL format:\n",
    "\n",
    "<p style=\"text-align: center;\">`https://www.dhi.ac.uk/api/data/oldbailey_record_single?idkey=`  </p>\n",
    "\n",
    "For example, https://www.dhi.ac.uk/api/data/oldbailey_record_single?idkey=t16740429-1 gives you the link to the XML file of the first proceeding in the database.\n",
    "\n",
    "\n",
    "**Get the XML file of the fifth trial in first_10.** A successful `.get` request returns `<Response [200]>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1581022295239,
     "user": {
      "displayName": "Violet Yao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB71gFX2MIVV9B2u98jEr6NxRAe3zZexUayLswnlg=s64",
      "userId": "10253555746784075040"
     },
     "user_tz": 480
    },
    "id": "UTuUSmdLPrbu",
    "outputId": "3dfe9e3b-a68a-49a7-a0a9-7c33a2e6f50c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fifth_trial_id = first_10[4]\n",
    "url = 'https://www.dhi.ac.uk/api/data/oldbailey_record_single?idkey={}'.format(fifth_trial_id)\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WffFWa9rPrbv"
   },
   "source": [
    "Run the next cell to see the XML format of the text! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1581022301672,
     "user": {
      "displayName": "Violet Yao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB71gFX2MIVV9B2u98jEr6NxRAe3zZexUayLswnlg=s64",
      "userId": "10253555746784075040"
     },
     "user_tz": 480
    },
    "id": "UaGyPfyjPrbw",
    "outputId": "e38f0bb3-5ab5-403d-a229-01bd5a6ed654"
   },
   "outputs": [],
   "source": [
    "response.json()['hits']['hits'][0]['_source']['xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pADUsBtjPrbx"
   },
   "source": [
    "We can save just the XML portion in a local file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 647,
     "status": "error",
     "timestamp": 1581022307915,
     "user": {
      "displayName": "Violet Yao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB71gFX2MIVV9B2u98jEr6NxRAe3zZexUayLswnlg=s64",
      "userId": "10253555746784075040"
     },
     "user_tz": 480
    },
    "id": "BPjM1haCPrbx",
    "outputId": "3d582eed-942a-4956-fd50-a5806460f3fe"
   },
   "outputs": [],
   "source": [
    "with open(f'data/old-bailey/old-bailey-{fifth_trial_id}.xml', 'w') as file:\n",
    "    file.write(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get trial `t17031013-13` specifically, for the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davis_trial_id = 't17031013-13'\n",
    "response = ...\n",
    "with open(f'data/old-bailey/old-bailey-{davis_trial_id}.xml', 'w') as file:\n",
    "    file.write(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KR7GETNQPrby"
   },
   "source": [
    "### Challenge: Scraping all trials from 1754 - 1756\n",
    "\n",
    "Now we have extracted trial IDs and trial XML trees using `requests.get(some_url)`, so we can iterate through each ID in a of trials (use `doc_ids` from above for the list of IDs). You can choose how many trials you want to save--maybe 30 to start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJN6UkcxPrb0"
   },
   "outputs": [],
   "source": [
    "for doc_id in doc_ids[:30]:\n",
    "    #format URL\n",
    "    url =  'https://www.dhi.ac.uk/api/data/oldbailey_record_single?idkey={}'.format(doc_id)\n",
    "    print(url)\n",
    "    #get the JSON from URL\n",
    "    tree = requests.get(url).json()\n",
    "    #save just the xml of the trial in the file\n",
    "    with open('data/old-bailey/old-bailey-' + doc_id + '.xml', 'w') as file:\n",
    "        file.write(...)\n",
    "    #one second pause so servers aren't overloaded\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAvriff2Prb2"
   },
   "source": [
    "You can check if you saved the XML files by executing the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2j2KSL8mPrb2"
   },
   "outputs": [],
   "source": [
    "!ls data/old-bailey/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtKSmptxPrb4"
   },
   "source": [
    "This cell will show you the XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-JSzqUBPrb5"
   },
   "outputs": [],
   "source": [
    "!cat data/old-bailey/old-bailey-t17540116-1.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TH1jtitPraW"
   },
   "source": [
    "## Section 2: XML Syntax<a id='section-2'></a>\n",
    "\n",
    "First, we'll go over the syntax of a XML file. The basic unit of XML code is called an \"element\" or \"node\" and has a start and ending tag. The tags for each element look something like this:\n",
    "\n",
    " `<exampletag>some text</exampletag>`  \n",
    "\n",
    "Run the next cell to look at the XML file of one of the cases from the OldBailey API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K9VKN8fPraY",
    "outputId": "84e2e1b5-62be-4569-8a58-689e02c0dda5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use requests to get an example of xml\n",
    "example = requests.get(f'https://www.dhi.ac.uk/api/data/oldbailey_record_single?idkey={davis_trial_id}').json()\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOIUR1SSPraf"
   },
   "source": [
    "A ha! Old Bailey Online revised the way it represents data. It used to be that each file contained just the XML, but now each one is a dictionary, among the keys of which is the XML. It is the XML we want to parse, so let's separate that out, like we did up above when we wrote out the Old Bailey Online files locally. You can see that the key 'xml' is shortly after the 'idkey' that we used when we made the list of trial records above. Let's inspect the XML from Samuel Davis's trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the xml from the Samuel Davis trial account\n",
    "example['hits']['hits'][0]['_source']['xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOIUR1SSPraf"
   },
   "source": [
    "The `interp` tags at the beginning of the file are elements that don't have any plain text content. Note that elements may possibly be empty and not contain any text (i.e. `interp` elements mentioned earlier). If the element is empty, the tag may follow a format that looks similar to `<exampletag/>`, which is equivalent to `<exampletag></exampletag>`.\n",
    "\n",
    "Elements may also contain other elements, which we call \"children\". Most children are indented, but the indents aren't necessary in XML and are used for clarity to show nesting. For example, if we go down to `<persName id=\"t17540116-4-defend46\" type=\"defendantName\">` , we see that the `rs` tag is a child of `persName`. We will explore about children in XML more in the next section. \n",
    "\n",
    "Lastly, elements may have attributes, which are in the format `<exampletag name_of_attribute=\"somevalue\">`. Attributes are designed to store data related to a specific elements. Attributes **must** follow the quotes format (`name = \"value\"`). As you can tell, in this XML file, attributes are everywhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkseGV_hPrah"
   },
   "source": [
    "-----\n",
    "**Question 2.1:** What was the verdict of this case? Was there a punsihment and if so, what was it? List both and state whether you found it as plain text content or as an attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9tJ_ejwPrai"
   },
   "source": [
    "_Write your answer here :_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZs0PgQmPrap"
   },
   "source": [
    "----\n",
    "## Section 3: Using Beautiful Soup to parse XML<a id='section-3'></a>\n",
    "\n",
    "Now that we know what the syntax and structure of an XML file, let's figure out how to parse through one! We are going to load the same file from the second section and use [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to navigate through elements in this file. \n",
    "\n",
    "First, we need to import the file into a Beautiful Soup instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZIKoc7dPraq"
   },
   "outputs": [],
   "source": [
    "xml_file = f'data/old-bailey/old-bailey-{davis_trial_id}.xml' #local file\n",
    "xml_file = open(xml_file).read()\n",
    "davis_trial_soup = BeautifulSoup(xml_file)\n",
    "print(davis_trial_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine `davis_trial_soup` using `.contents`, which puts all children of a tag in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "davis_trial_soup.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that all information we care about is contained within `<div>` and `</div1>` tags, so we navigate to it. The simplest way to navigate the parse tree is to say the name of the tag you want (`.`). In this case, we want to access div1 under body tag, which is under html tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = davis_trial_soup.html.body.div1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start working down the tree! With the body, we can find each child of the body by printing the tags. This will also help us for future reference, if we every want to go through other children in the XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in body.children:\n",
    "    if child.name:\n",
    "        print(child.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of children to work with let's select one using `.`. Using `.` navigates through the hierarchical structure of XML and helps us keep track of the path we are taking through this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_p = body.p\n",
    "for child in choose_p.children:\n",
    "    if child.name:\n",
    "        print(child.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't very helpful, since we're still left with a bunch of tags and on top of that, we have a lot of repeating tags and names. Let's choose `placename` as our next tag and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name = body.p.placename\n",
    "for child in place_name.children:\n",
    "    if child.name:\n",
    "        print(child.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing was printed, so it looks like we hit the end! Let's use `.string` to examine the data in this element, following the `.` path we used to get here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body.p.placename.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** Find the defendant's name by traversing through the correct elements. You can check your answer with printed XML using `soup.contents`\n",
    "\n",
    "You may find `body.p.persname.string` returns None. If a tag, `body.p.persname.string` in this case, contains more than one thing, then it’s not clear what .string should refer to, so .string is defined to be None. Which functions could help us locate the name instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B08ehQmMPrbN",
    "outputId": "f0158484-bf40-4dc1-d357-05b3c9772f2d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0KUeGviPrbg"
   },
   "source": [
    "**Question 2.2:** Since the textual data is pretty messy in the XML files of these proceedings, where do you think the data you need might be held and how might you go about extracting this data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR3n0Gu6Prbh"
   },
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60oOKGf9Prb6"
   },
   "source": [
    "----\n",
    "## Section 4: Putting it all in a dataframe<a id='section-4'></a>\n",
    "\n",
    "Now that we have a bunch of XML files and know how to parse through them to extract data, let's put the data from the XML files into a dataframe. Take a look at the XML for [this trial](https://www.oldbaileyonline.org/record/t17031013-3?text=t17031013-3) (and even better, look at what is or isn't consistent between that one and some others), and think about the structure of the data. How would you identify the people involved in a case? How would you identify their roles (witness/defendant/victim/other), or their genders? What can you learn about the alleged offence?\n",
    "\n",
    "*Note:* Some cases have multiple defendants, multiple victims or multiple witnesses; however, most cases only have at most one of each. You can represent this in a dataframe by having $N$ columns for each property of a defendant, victim, etc., but this results in many many empty cells, and may not be amenable to analysis for the questions you come up with.\n",
    "\n",
    "Think about the kinds of questions you may want to ask about this data, and refer to the XML for how you might answer them. For example, you may be interested in\n",
    "\n",
    "- the words used specifically in describing the crime (notice that the text specifically between `<rs id=\"...\" type=\"offenceDescription\">` and `</rs>` gives you this)\n",
    "\n",
    "- whether any victim was female\n",
    "\n",
    "- whether any defendant was female\n",
    "\n",
    "- the `category` (or `subCategory`) of the offense, etc.\n",
    "\n",
    "- the entire text of the trial (sans tags)\n",
    "\n",
    "These are questions that can be answered for most if not all cases, so they make good candidates for names of columns.\n",
    "\n",
    "**Question 4.1**: Start by completing the following function to get the date of the case and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_date(case_soup):\n",
    "    for element in case_soup.body.div1.contents:\n",
    "        if element.name == ...:\n",
    "            if element.attrs['type'] == ...:\n",
    "                return ...\n",
    "            \n",
    "print(\"Case\", davis_trial_id, \"happened on\", case_date(davis_trial_soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2**: Complete the following function finding every person in a trial, and returning a list of dictionaries of their attributes (e.g. `[{\"surname\": \"FINCH\", \"given\": \"JOHN\", \"gender\": \"male\", \"type\": \"witnessName\"}]`). Test it on `davis_trial_soup` used before. **Note:** If you use `find_all`, specify the tag name in lowercase, as beautifulsoup lowercases all tag names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_in_case(case_soup):\n",
    "    people = []\n",
    "    for persName in case_soup.body.div1.find_all('persname'):\n",
    "        person = {}\n",
    "        person[\"type\"] = persName.attrs.get(\"type\") # `thing.get(key)` is like `thing[key]` but returns None if key is not in x instead of raising an exception\n",
    "        for interp in persName.find_all('interp'):\n",
    "            fieldName = ...\n",
    "            fieldValue = ...\n",
    "            person[fieldName] = fieldValue\n",
    "        people.append(person)\n",
    "    return people\n",
    "\n",
    "print(\"Case\", davis_trial_id, \"describes these people:\\n\", people_in_case(davis_trial_soup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3**: Complete the following function to find the `offenseDescription` and `verdictDescription` in a trial. Think about how the XML expresses the offenseDescription and verdictDescription, and see if you can write the code without specifically looking for the labels \"offenseDescription\" and \"verdictDescription\" (i.e. so that it will work even if a case came up with something like `<rs type=\"sentencingDescription\">`). Get the category, subCategory and textual description of the offense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: myString.strip() removes the whitespace at the beginning and end of myString\n",
    "\n",
    "def case_descriptions(case_soup):\n",
    "    descriptions = {}\n",
    "    for rs in ...:\n",
    "        desc = {}\n",
    "        for interp in ...:\n",
    "            fieldName = ...\n",
    "            fieldValue = ...\n",
    "            desc[fieldName] = ...\n",
    "        desc[\"text\"] = ... \n",
    "        descriptions[rs.attrs['type']] = desc\n",
    "    return descriptions\n",
    "print(\"Case\", davis_trial_id, \"has these various descriptions in <rs> elements:\\n\", case_descriptions(davis_trial_soup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4:** Once you get this far, you've learned how to parse most of the data provided in the Old Bailey XML. Now, think about the data you now have access to for each case, and complete the following function creating a dataframe describing all of trials in `trials[\"hits\"][:100]`.\n",
    "\n",
    "One easy way to do this is to make a list of dictionaries, and pass this to `pd.DataFrame`, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{\"x\": 1, \"y\": 10}, {\"y\": 12, \"z\": 111}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the questions you may want to ask about the data, and complete the following function to put it all together in a DataFrame:\n",
    "\n",
    "*Note!:* This is not easy. Take it one step at a time, initially just making one a DataFrame with one column, and building up from there. I made several little errors while writing it up (take note of capitalization in property names like `offenceSubcategory` and to spell it 'offence' not 'offense' (i.e. the British way)). **The rewarding thing** is that once you've written this up, as you come up with new questions to ask about the case data you'll be able to easily add columns to use in your analysis.\n",
    "\n",
    "*If you are stuck*, try looking at the *data output* of the solutions (avoid looking at the code until you've worked through it), picking *one* column, and thinking of how you can answer that with the functions you've made or learned already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan\n",
    "def table_of_cases(xml_file_names):\n",
    "    rows = []\n",
    "    for xml_file in xml_file_names:\n",
    "        with open(f'data/old-bailey/old-bailey-{xml_file}.xml', \"r\") as xml_file:\n",
    "            case = BeautifulSoup(xml_file)\n",
    "        people = ...\n",
    "        date = ...\n",
    "        descriptions = ...\n",
    "        row = {\n",
    "            \"date\": date,\n",
    "            \"id\": case.div1.attrs[\"id\"],\n",
    "            \"text\": \" \".join(case.text.split()), # split on all whitespace, then join on \" \", to remove long sequences of whitespace\n",
    "            \"any_defendant_female\": False,\n",
    "            \"any_defendant_male\": False,\n",
    "            \"any_victim_female\": False,\n",
    "            \"any_victim_male\": False,\n",
    "        }\n",
    "        if \"offenceDescription\" in descriptions:\n",
    "            row[\"offenceText\"] = descriptions[\"offenceDescription\"].get(\"text\", nan) # `dictionary.get(key, default)` is the same as `dictionary[key] if key in dictionary else default`\n",
    "            row[\"offenceCategory\"] = ...\n",
    "            row[\"offenceSubcategory\"] = ...\n",
    "        if \"verdictDescription\" in descriptions:\n",
    "            row[\"verdictText\"] = ...\n",
    "            row[\"verdictCategory\"] = ...\n",
    "        if \"punishmentDescription\" in descriptions:\n",
    "            row[\"punishmentText\"] = ...\n",
    "            row[\"punishmentCategory\"] = ...\n",
    "            row[\"punishmentSubcategory\"] = ...\n",
    "        for person in people:\n",
    "            if person.get(\"type\") == \"defendantName\" and person.get(\"gender\") == \"female\":\n",
    "                row[\"any_defendant_female\"] = True\n",
    "            if person.get(\"type\") == ...\n",
    "            if person.get(\"victim\") == ...\n",
    "            if person.get(\"victim\") == ...\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "table_of_cases(doc_ids[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIjKFhrJPrcC"
   },
   "source": [
    "Note that the document that was front matter in the printed Old Bailey Proceedings has NaN in the `offenceCategory` column--it is not a document with a criminal trial record. You could easily eliminate the rows that are not trial records. \n",
    "\n",
    "Phew, that's it! Now you know how to parse through XML files using Beautiful Soup and web scrape using the `requests` library! This was a long lab, so pat yourself on the back for working through it. It will help you on problem set 3 enormously.\n",
    "\n",
    "You will note that the lab does not have much to fill in, relative to the solution file. Working with the Old Bailey Online data is challenging, so **ask questions when you don't quite get what's happening**. You can test your understanding by adding extending the solutions with more columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRwXZDiDPrcD"
   },
   "source": [
    "## Bibliography\n",
    "\n",
    " - All files from Old Bailey API - https://www.oldbaileyonline.org/obapi/\n",
    " - ElementTree information adapted from Driscoll, Mike. (2013, April). Python 101 – Intro to XML Parsing with ElementTree.\n",
    " https://www.blog.pythonlibrary.org/2013/04/30/python-101-intro-to-xml-parsing-with-elementtree/\n",
    "\n",
    " - Web Scraping code adapted from MEDST-250 Notebook developed by Tejas Priyadarshan.\n",
    " https://github.com/ds-modules/MEDST-250/tree/master/04%20-%20XML_Day_1\n",
    " \n",
    " - Image source from https://www.researchgate.net/publication/257631377_Efficient_XML_Path_Filtering_Using_GPUs\n",
    " \n",
    " - Beautiful Soup 4 [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "----\n",
    "Notebook developed by: Jason Jiang, Iland Leigh, Violet Yao, and Wilson Berkow; adjustment to new Old Bailey Online API by Jon Marshall 2024\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KR7GETNQPrby",
    "60oOKGf9Prb6",
    "VRwXZDiDPrcD"
   ],
   "name": "BS WIP Parsing_XML_Data_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
