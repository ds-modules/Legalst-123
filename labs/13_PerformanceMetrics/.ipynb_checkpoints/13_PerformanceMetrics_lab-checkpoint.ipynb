{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360a1e0f",
   "metadata": {},
   "source": [
    "## Lab 13: Performance Metrics for Classifiers\n",
    "In Data 8 students learn about performance metrics for continuous outcomes; the most prominent of these was Root Mean Square Error (RMSE), or how far away on average the model's prediction is from the actual value of the outcome. For classifers it is a bit more complicated, but we still want to know how well our model performed. \n",
    "\n",
    "Here we will use one year of Nashville traffic stop data--2013, sort of the \"high tide\" of traffic stops on the part of the Nashville police, who recorded almost 400k stops that year. We will return to what we were working on in Lab 11 and try to predict searches and measure how well our classifer does. Then we will add geospatial data (in the form of census block group, since we do not have a map of organic Nashville neighborhoods) to see if we can get a better prediction of whether or not a stop resulted in a search. Finally, we will take advantage of the geospatial data for a map visualization of searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.decomposition import PCA\n",
    "import sklearn.linear_model as linear\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import seaborn as sns\n",
    "!pip install geopandas\n",
    "!pip install rtree\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "!pip install plotly\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import display, HTML # makes the output in Jupyter notebook pretty\n",
    "import plotly.express as px\n",
    "import json\n",
    "import os\n",
    "\n",
    "# set the random seed so that results reproduce\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd66be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6c2e41",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "For this lab we will use the file of Nashville stop data, rather than the sample used in the previous two labs. We will do what we did before to ready the data for fitting the model.\n",
    "* read the datafile for 2013 into a dataframe\n",
    "* clean the data and deal with any missing data that will affect the analysis\n",
    "* select the features you used in Lab 11 and make them into a dataframe\n",
    "* split the data into training, validation, and test sets\n",
    "* scale the data\n",
    "\n",
    "The 2013 data will occupy more memory than the sample but we still should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c43041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned data for 2013 only\n",
    "\n",
    "path = \"https://github.com/jdmarshl/data/raw/main/2013_stops_cleaned.csv\"\n",
    "stops_2013 = pd.read_csv(path, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b422370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stops_2013.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find proportions of missing data\n",
    "\n",
    "pd.options.display.float_format = '{:.8f}'.format # use 8 decimal places and not scientific notation for float\n",
    "stops_2013..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there any problematic age data?\n",
    "stops_2013[stops_2013['subject_age'] > 90]['subject_age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b60bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use rows where the subject_age is less than 99\n",
    "\n",
    "stops_2013 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e5949",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Other than age, is there any feature we need to be concerned with at this point? Why is it important to get rid of the anomalous `'subject_age'` values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452bae7",
   "metadata": {},
   "source": [
    "_your answer here_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d895177",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46129940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features that you arrived at in Lab 11 (you can adjust those if you want) \n",
    "# to use for predicting search but be sure to include at least the ones listed below\n",
    "# since we want to see how the model predictions are affected by subject race\n",
    "\n",
    "# note dummy variables have a reference category, and there is a collinearity problem \n",
    "# if *all* the dummy variables are used as predictors at once\n",
    "reasonable_features = ...\n",
    "reasonable_df = stops_2013[reasonable_features]\n",
    "reasonable_matrix = reasonable_df.corr()\n",
    "plt.figure(figsize=(len(reasonable_features),len(reasonable_features)))\n",
    "g = sns.heatmap(reasonable_matrix, annot=True, annot_kws={\"size\": 12})\n",
    "reasonable_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17434b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "y = ...\n",
    "X = ...\n",
    "\n",
    "# split the sampled data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.80, test_size=0.20)\n",
    "# split the sampled training set into training and validation sets\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, train_size=0.75, test_size=0.25)\n",
    "\n",
    "# reminder of size of training, validation, and test sets\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_validate shape: \", X_validate.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform all the sets based on the the scaler fit to the training data and make them into df's\n",
    "# note: you can fit and transform the training set in one step if you want\n",
    "\n",
    "X_train = ...\n",
    "X_validate = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685f73a",
   "metadata": {},
   "source": [
    "#### Baseline or \"naive\" prediction\n",
    "\n",
    "If someone were to ask you \"Did this randomly drawn traffic stop result in a search,\" what should your answer be? What is the accuracy of the baseline prediction of \"no search\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d46c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the baseline accuracy on the validation to compare results to later\n",
    "print(\"Baseline accuracy (proportion search_conducted = False) on validation set is \", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdbfdd",
   "metadata": {},
   "source": [
    "## 2. Construct the logistic regression model\n",
    "\n",
    "We will again use sklearn's logistic classifier. It will be very hard to beat a naive prediction of \"no search\" because searches are so uncommon. Because we will want to penalize weak predictors once we add a large number of geospatial features (census block groups), this time we will apply $l2$ regularization to the model. This means that rather than minimizing just the sum of the squared residuals (RSS), we are minimizing the sum of the squared residuals plus a shrinkage penalty. In sklearn's [logistic classifier](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), and in Ridge regression for making estimates of continuous outcome, the penalty is the $l2$ norm. Here the shrinkage penalty is the product of a tuning parameter $\\lambda$ and the sum of the squared coefficients. This has the effect of shrinking small coefficients toward zero. Here is the objective function to minimize in math notation.\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\left( y_{i} - \\beta_{0} - \\sum_{j=1}^{p} \\beta_{j}x_{ij} \\right) ^2 + \\lambda \\sum_{j=1}^{p} \\beta_{j}^{2} = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2\n",
    "$$\n",
    "\n",
    "\n",
    "Also, we will use `'class_weight' =  'balanced'` to adjust for the fact that searches are infrequent. We should still keep our enthusiasm in check, because it does not seem as though any of our predictors will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aead26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a logistic classifier model using sklearn\n",
    "\n",
    "logit_cf = ...\n",
    "\n",
    "# fit the logistic regression model to the training set\n",
    "logit_cf.fit(X_train, y_train)\n",
    "\n",
    "# check the accuracy on the training and validation sets\n",
    "print(\"Accuracy on training set: \", ...)\n",
    "print(\"Accuracy on validation set: \", ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b06ee3",
   "metadata": {},
   "source": [
    "## 3. Gauge the performance of the model\n",
    "All alone, accuracy does not tell us everything we need to know about the performance of a classifier.\n",
    "\n",
    "For a well-developed explanation (using the language of linear algebra) see the [Data 100 lecture on logistic classifer thresholds and performance measures](https://ds100.org/fa23/resources/assets/lectures/lec23/lec23.html)\n",
    "\n",
    "This model was far less accurate than the naive prediction. Typically, though, we want to know not just what proportion the model got right, but how good the model is at classifying both $Y = 1$ and $Y = 0$ classes. That is, we want to know True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). This can be visualized in a confusion matrix and summarized by the following performance measures\n",
    "* Precision = $\\frac{TP}{TP + FP}$ increases as the number of false positives decreases, as the classification threshold is raised. One way to think of Precision is as what proportion of the class members the classifier identifies are actually members of that class.\n",
    "\n",
    "* Recall = $\\frac{TP}{TP +FN}$ increases as the number of false negatives decreases, as the classification threshold is lowered. One way to think of Recall is as what proportion of the total number of class members does the classifier identify.\n",
    "\n",
    "* AUC-ROC is the Area Under the Curve - Receiver Operating Characteristic. This is a summary measure of how good a job the classifier does distinguishing between positive and negative class membership at all threshold settings. For example, if we choose one instance at random from the positive class and one at random from the negative class, an AUC-ROC score of 0.6 means that there is a 0.6 probability that the positive instance got a higher score from the classifier.\n",
    "\n",
    "We will also take advantage of the fact that the logistic classifier returns a probability, and then compares that to a threshold value (for sklearn linear logistic classifier) to predict class membership. Below we will examine the AUC-ROC score, whether the predicted probability of membership in the positive class differs by race, and also what the coefficient of each feature is. \n",
    "\n",
    "For more on model performance metrics see the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).\n",
    "\n",
    "Note that `.predict_proba` returns an array in which the first col is the probability of False and the second column is the probability of True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to compute statistics across predictors (credit: Vyoma Raman)\n",
    "def demographics(df, col):\n",
    "    total = df[col].mean()\n",
    "    black = df[df.subject_race_black > 0][col].mean()\n",
    "    nonblack = df[df.subject_race_black <= 0][col].mean()\n",
    "    male = df[df.subject_sex_male > 0][col].mean()\n",
    "    nonmale = df[df.subject_sex_male <= 0][col].mean()\n",
    "    moving_violation = df[df['violation_moving traffic violation'] > 0][col].mean()\n",
    "    equipment_violation = df[df['violation_vehicle equipment violation'] > 0][col].mean()\n",
    "    investigative_stop = df[df['violation_investigative stop'] > 0][col].mean()\n",
    "    print(col, 'average for all stops:', round(total, 5))\n",
    "    print(col, 'average for black drivers:', round(black, 5))\n",
    "    print(col, 'average for non-black drivers:', round(nonblack, 5))\n",
    "    print(col, 'average for male drivers:', round(male, 5))\n",
    "    print(col, 'average for non-male drivers:', round(nonmale, 5))\n",
    "    print(col, 'average for moving violations:', round(moving_violation, 5))\n",
    "    print(col, 'average for equipment violations:', round(equipment_violation, 5))\n",
    "    print(col, 'average for investigative stops:', round(investigative_stop, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model prediction and the probability estimate for search for each observation in validation set\n",
    "predictions = ...\n",
    "probabilities = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df that has the predictors, model predictions, and probabilities for the validation set\n",
    "compare = X_validate.copy()\n",
    "compare['prediction'] = list(predictions) # need to cast as a list because the array above has wrong shape\n",
    "compare['probability'] = probabilities[:, 1] # the element of the array for probability of true\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86860d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the average predicted probability of a search by race and type of stop\n",
    "demographics(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73574c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what proportion of the demographic groups does the model predict will be searched?\n",
    "demographics(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a887304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's illustrate the prediction accuracy with a confusion matrix for the validation set\n",
    "\n",
    "logit_cf_matrix = metrics.confusion_matrix(...)\n",
    "logit_cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40321807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the confusion matrix as a heatmap\n",
    "\n",
    "#SOLUTION\n",
    "logit_cf_cm = pd.DataFrame(logit_cf_matrix, range(2), range(2))\n",
    "logit_cf_cm = logit_cf_cm.rename(index=str, columns={0:'False', 1:'True'})\n",
    "logit_cf_cm.index = ['False', 'True']\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(logit_cf_cm, \n",
    "           annot=True,\n",
    "           fmt = '9.0f',\n",
    "           annot_kws={\"size\": 16})\n",
    "\n",
    "plt.title(\"Logistic Classifier Confusion Matrix for Search Conducted\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the logistic regression classifier's recall and precision and AUC-ROC score on the validation set\n",
    "\n",
    "logit_cf_recall = ...\n",
    "logit_cf_precision = ...\n",
    "logit_cf_auc_roc_score = ...\n",
    "print(\"logistic regression classifer recall: \",logit_cf_recall)\n",
    "print(\"logistic regression classifer precision: \", logit_cf_precision)\n",
    "print(\"logistic regression classifer AUC-ROC score: \", logit_cf_auc_roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show which features are most predictive of search (remember that the features have been standardized \n",
    "# and it is a logistic regression so you cannot interpret the coefficients as the actual increase in probability)\n",
    "# you can just zip the features and the .coef_ attribute of the model\n",
    "\n",
    "feature_coefs = zip(...)\n",
    "list(feature_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aba684",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How good was the logistic regression classifier at predicting whether or not there was a search after a traffic stop? Explain with reference to the metrics above.\n",
    "2. What could you conclude about the logit classifier if you just looked at the AUC-ROC score? \n",
    "3. Write down two interesting pieces of information about predicting search from the exercise above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468fa75",
   "metadata": {},
   "source": [
    "_your answers here_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab88dd",
   "metadata": {},
   "source": [
    "## 4. Add geospatial data from American Community Survey and add features\n",
    "So far we have had a tough time predicting searches, but the Nashville traffic stop data has a limited list of features. The literature on policing says that police behavior varies by location, and that police search more in 'high crime' areas. There is also a body of scholarship that says police are more willing to question people when they seem out of place--the \"race out of place\" hypothesis. It seems reasonable to see if adding some geospatial and demographic information might help us.\n",
    "\n",
    "Because San Francisco robbed us of a chance to do a geospatial join using geopandas, we will do that here to put each traffic stop into a census block group. That will allow us to see some basic information about the block group like median income and racial composition. I used American Community Survey data from 2013 to add the median income data and data on race. Since Nashville uses different categories for race, I did my best to approximate them. The ACS codes race by asking participants about race and Hispanic ethnicity. In the file that we will use below I coded race as a police officer would ascribe race to someone--regardless of Hispanic ethnicity, white, Black, and API were coded in those catgories, while ACS's category of Hispanic and two or more races was coded as Hispanic. I am not certain but I don't think Nashville police in 2013 asked drivers whom they stopped what their race was--they just filled in a race on a reporting form. So I am assuming it is the police officer making the report who is constructing the \"race\" category in Nashville.\n",
    "\n",
    "According to the Census Bureau, the coordinate reference system for the Census Block Groups is GRS_1980, which for GeoPandas is EPSG:7019, but the GeoPandas [reference on coordinate reference systems](https://geopandas.org/en/stable/docs/user_guide/projections.html) says that most of the time you don't need to since data from reputable sources include projection information. It also does not recognize that CRS code. The debugger said that the census block group coordinate reference system is 'EPSG:4269'. We will stick with that  for now. We will have to use [GeoSeries](https://geopandas.org/en/stable/docs/reference/geoseries.html) to make a geometry column for the dataframe with the traffic stop data in it.\n",
    "\n",
    "There is a risk that we will bump up against memory quotas, or that GeoPandas may not work as expected, but I am keeping my fingers crossed. **Finally, we should be sure that all of the features are of the right type to pass to sklearn. More below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have latitudes and longitudes but any mapping is going to want a list of coord pair lists so keep that\n",
    "\n",
    "lats = ...\n",
    "longs = ...\n",
    "coords = ...\n",
    "reasonable_df['coords'] = coords\n",
    "reasonable_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to create a GeoSeries geometry object to add the points to our dataframe\n",
    "# as a shapely.Point geometry, although with .from_xy method\n",
    "# Note: GeoSeries expects x to be longitude and y to be latitude!\n",
    "\n",
    "from shapely.geometry import Point\n",
    "x = ...\n",
    "y = ...\n",
    "geopoints = gpd.GeoSeries.from_xy(x, y, crs='EPSG:4269')\n",
    "len(geopoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d1e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn the pandas dataframe of selected features into a GeoDataFrame by adding the geometry\n",
    "# column we made with GeoSeries\n",
    "\n",
    "reasonable_gdf = gpd.GeoDataFrame(...)\n",
    "reasonable_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3160b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file of Nashville block groups and associated demographic features into GeoDataFrame\n",
    "path = 'https://github.com/jdmarshl/data/raw/main/nashville_block_groups.shp'\n",
    "block_groups = gpd.read_file(path)\n",
    "block_groups.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75f16f",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Looks like some columns need to be cast as different types!**</span>\n",
    "\n",
    "The ACS data are stored as strings, and so we have to do something to check for missing data and turn the strings into numbers. In particular, we know we want the 2013 median household income `'med_inc_20'` to be an integer and the proportions of populations in the various Nashville racial groups `'white_pct', 'black_pct',v'hispanic_p', 'asianpac_1', 'other_pct'` to be floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the median incomes are strings, we should remove any rows that are spurious\n",
    "block_groups = block_groups.where(block_groups.med_inc_20.str.len() > 3)\n",
    "block_groups.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455efafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows that now have NaNs for 'med_inc_20' and cast the columns as the appropriate data type\n",
    "# using the pandas method .to_numeric\n",
    "\n",
    "block_groups.dropna(inplace=True)\n",
    "block_groups['med_inc_20'] = ...\n",
    "...\n",
    "block_groups.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb394b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_blk_groups = block_groups.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d803f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(just_blk_groups, x='med_inc_20')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde6504",
   "metadata": {},
   "source": [
    "### Joining the traffic stop data and the geospatial data\n",
    "We now need to connect the traffic stop data, which are represented as points in space. To do this we will use the [GeoPandas spatial join method](https://geopandas.org/en/stable/docs/user_guide/mergingdata.html). Note that there are a number of ways to [make a spatial join](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.sjoin.html#geopandas.GeoDataFrame.sjoin).\n",
    "\n",
    "The GeoPandas spatial join will allow us to place the point geometry of each traffic stop in the proper polygon that represents the census block group. We can then tie the traffic stop information to the demographics of the block in which it occurred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcad000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial join of the two dataframes defined by polygons and points\n",
    "block_groups = block_groups.sjoin(...)\n",
    "block_groups.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4936e",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "How many rows were in `'reasonable_df'`? How many rows are in the merged GeoDataFrame `'block_groups'`? How do you account for the discrepancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2e389",
   "metadata": {},
   "source": [
    "_your answer here_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc050ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a scatterplot of latitudes and longitude can help answer the question above\n",
    "\n",
    "fig = px.scatter(...)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0a9ad",
   "metadata": {},
   "source": [
    "## 5. Gauge performance of model with added features\n",
    "Now (with a fair amount of effort) we have added some environmental features, like median household income, of the geographic area of the traffic stop to the individual-level features we had, like `'subject_race'`. We can see if adding the environmental features of a traffic stop improves our ability to predict whether or not the stop led to a search.\n",
    "\n",
    "This time you may just use the features you used before, which are now in the GeoDataFrame, and add at least the median household income and proportion Black residents as census block group level features. After that, split the data, scale the data, and fit the logistic classifier to the new training set. We can then see whether the performance metrics have improved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b409dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_groups.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032deb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features as above but add the environmental features from the census block groups\n",
    "\n",
    "reasonable_features = [...]\n",
    "reasonable_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "y = ...\n",
    "X = ...\n",
    "\n",
    "# split the sampled data into training and test sets\n",
    "\n",
    "# split the sampled training set into training and validation sets\n",
    "\n",
    "\n",
    "# reminder of size of training, validation, and test sets\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_validate shape: \", X_validate.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data on the training set and apply the scaling to validation and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the logistic regression model to the training set\n",
    "# then report the accuracy on the training and validation sets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model prediction and the probability estimate for search for each observation in validation set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df that has the predictors, model predictions, and probabilities for the validation set\n",
    "\n",
    "compare = X_validate.copy()\n",
    "compare['prediction'] = ...\n",
    "compare['probability'] = ...\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the prediction accuracy with a confusion matrix for the validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the confusion matrix as a heatmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the logistic regression classifier's recall and precision and AUC-ROC score on the validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show which features are most predictive of search (remember that the features have been standardized so you\n",
    "# cannot interpret the coefficients as the actual increase in probability)\n",
    "\n",
    "feature_coefs = \n",
    "list(feature_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9354b7",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. Did the model perform any better with the environmental features included? \n",
    "2. Can you think of other ways to improve the performance of the model?\n",
    "3. What did this (extended!) exercise teach us about predicting police searches of cars in traffic stops?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca876ec",
   "metadata": {},
   "source": [
    "_your answers here_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11167d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
